{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mislavz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mislavz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mislavz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialization for HateSpeechRecognition\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import t\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_kfold_cv(clf, param_grid, X, y, k1=10, k2=3):\n",
    "    \n",
    "    err = []\n",
    "    kfold = KFold(n_splits=k1, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Outer loop\n",
    "    for ind_train, ind_test in kfold.split(X):\n",
    "        \n",
    "        X_train, y_train, X_test, y_test = X[ind_train], y[ind_train], X[ind_test], y[ind_test]\n",
    "        \n",
    "        # Inner loop\n",
    "        inn = GridSearchCV(clf,param_grid,n_jobs=5, cv=StratifiedKFold(n_splits=5, \n",
    "                                              random_state=42).split(X_train, y_train), \n",
    "                           verbose=2).fit(X_train, y_train)\n",
    "        \n",
    "        # Prediction based on the best selected params, the ones that minimize average error\n",
    "        h = inn.best_estimator_.fit(X_train, y_train).predict(X_test)\n",
    "        \n",
    "        err.append(zero_one_loss(y_test, h))\n",
    "        print(classification_report( y_test, h ))\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(rgx_list, text):\n",
    "    new_text = text\n",
    "    for rgx_match in rgx_list:\n",
    "        new_text = re.sub(rgx_match, '', new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_words(content):\n",
    "    ret_list = []\n",
    "    fdist2 = nltk.FreqDist(content)\n",
    "    most_list = fdist2.most_common(75)\n",
    "    for x in most_list:\n",
    "        ret_list.append(x)\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "#other_exclusions = [\"#ff\", \"ff\", \"rt\",\"!\",\":\",\"...\",\".\",\"-\",\"&\",\"?\"]\n",
    "#stopwords_list.extend(other_exclusions)\n",
    "\n",
    "dataset = pd.read_csv(\"dataset/labeled_data.csv\")\n",
    "\n",
    "tweets = dataset.tweet\n",
    "\n",
    "ps = nltk.stem.PorterStemmer()\n",
    "\n",
    "#ps.stem(dataset)\n",
    "\n",
    "#Text cleaning and tokenization, then stemming then POS tagging\n",
    "filtered_tweets=[];\n",
    "tweet_tags = [];\n",
    "filtered_tweets_stemmed=[];\n",
    "common_words_prepare=[];\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweet = clean_text([\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\".*@.*:\",\"&#*\\w*\",\"@[\\w\\-]+\",\"[^\\w\\s]\"],tweet)\n",
    "    tweet = tweet.lower()\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    common_words = get_common_words(word_tokens)\n",
    "    for word in common_words:\n",
    "        common_words_prepare.append(word)\n",
    "    filtered_tweets.append([word for word in word_tokens if not word in stopwords_list])\n",
    "    filtered_tweets_stemmed.append([ps.stem(word) for word in word_tokens if not word in stopwords_list])\n",
    "    \n",
    "for tweet in filtered_tweets:\n",
    "    tweet_tags.append(nltk.pos_tag(tweet))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Izbacivanje frequent wordsa\n",
    "\n",
    "common_words = get_common_words(common_words_prepare)\n",
    "#for word in filtered_tweets[1:100]:\n",
    "#    print(word)\n",
    "common = []\n",
    "for word in common_words[1:50]:\n",
    "   common.append(word[0][0])\n",
    "\n",
    "filtered_tweets_no_common = [];\n",
    "filtered_tweets_no_common_stemmed = [];\n",
    "for line in filtered_tweets:\n",
    "    filtered_tweets_no_common.append([word for word in line if not word in common])\n",
    "    filtered_tweets_no_common_stemmed.append([ps.stem(word) for word in line if not word in common])\n",
    "#print(\"---------------------------\")\n",
    "#for word in filtered_tweets_no_common[1:100]:\n",
    "#    print(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating TfIdf vectorizer\n",
    "ftss=[]\n",
    "for tweet in filtered_tweets_no_common_stemmed:\n",
    "    ftss.append(' '.join(tweet))\n",
    "\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(ngram_range=(1,3),lowercase=False,max_features=1000,smooth_idf=False,norm=None,max_df=0.75,min_df=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(ftss).toarray()\n",
    "Y = dataset['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=42, test_size=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 69.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.12      0.19       164\n",
      "          1       0.84      0.97      0.90      1905\n",
      "          2       0.79      0.44      0.56       410\n",
      "\n",
      "avg / total       0.80      0.82      0.80      2479\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 69.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.38      0.16      0.22       127\n",
      "          1       0.84      0.97      0.90      1927\n",
      "          2       0.82      0.42      0.56       425\n",
      "\n",
      "avg / total       0.82      0.83      0.81      2479\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 67.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.10      0.17       136\n",
      "          1       0.84      0.97      0.90      1916\n",
      "          2       0.81      0.45      0.58       427\n",
      "\n",
      "avg / total       0.82      0.84      0.81      2479\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 69.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.14      0.21       145\n",
      "          1       0.85      0.97      0.90      1917\n",
      "          2       0.79      0.48      0.59       416\n",
      "\n",
      "avg / total       0.82      0.84      0.81      2478\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 70.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.16      0.26       171\n",
      "          1       0.83      0.98      0.90      1894\n",
      "          2       0.83      0.41      0.55       413\n",
      "\n",
      "avg / total       0.83      0.83      0.80      2478\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 67.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.15      0.23       141\n",
      "          1       0.85      0.97      0.91      1930\n",
      "          2       0.85      0.49      0.62       407\n",
      "\n",
      "avg / total       0.83      0.85      0.82      2478\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 69.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.16      0.25       154\n",
      "          1       0.84      0.97      0.90      1915\n",
      "          2       0.80      0.45      0.57       409\n",
      "\n",
      "avg / total       0.82      0.83      0.81      2478\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 66.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.13      0.21       135\n",
      "          1       0.84      0.97      0.90      1907\n",
      "          2       0.82      0.45      0.58       436\n",
      "\n",
      "avg / total       0.82      0.83      0.81      2478\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 72.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.13      0.21       133\n",
      "          1       0.86      0.97      0.91      1937\n",
      "          2       0.81      0.49      0.61       408\n",
      "\n",
      "avg / total       0.84      0.85      0.82      2478\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 68.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.42      0.13      0.20       124\n",
      "          1       0.86      0.98      0.91      1942\n",
      "          2       0.86      0.50      0.63       412\n",
      "\n",
      "avg / total       0.84      0.85      0.83      2478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SVM Classifier\n",
    "C = [ 2e-2, 2e-1, 2e-0]\n",
    "gama = [ 2e-2, 2e-1, 2e-0]\n",
    "param = [{'svc__kernel': ['rbf'], 'svc__C': C}]\n",
    "clf = svm.SVC(decision_function_shape='ovo')\n",
    "scale = StandardScaler()\n",
    "pipeline = Pipeline([('scaler', scale), ('svc', clf)])\n",
    "\n",
    "svm_err = nested_kfold_cv(pipeline, param, X, Y)\n",
    "#param_grid = [{}] \n",
    "#grid_search = GridSearchCV(pipeline, \n",
    "#                           param_grid,\n",
    "#                          n_jobs = 5,\n",
    "#                           cv=StratifiedKFold(n_splits=5, \n",
    "#                                              random_state=42).split(X_train, y_train), \n",
    "#                           verbose=2)\n",
    "#model = grid_search.fit(X_train,y_train)\n",
    "#y_pred = model.predict(X_test)\n",
    "#report = classification_report( y_test, y_pred )\n",
    "#print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.10      0.17       164\n",
      "          1       0.85      0.97      0.90      1905\n",
      "          2       0.80      0.54      0.65       410\n",
      "\n",
      "avg / total       0.82      0.84      0.81      2479\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.47      0.12      0.19       127\n",
      "          1       0.86      0.97      0.91      1927\n",
      "          2       0.80      0.50      0.62       425\n",
      "\n",
      "avg / total       0.83      0.85      0.82      2479\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.10      0.17       136\n",
      "          1       0.86      0.97      0.91      1916\n",
      "          2       0.81      0.55      0.66       427\n",
      "\n",
      "avg / total       0.83      0.85      0.83      2479\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.15      0.24       145\n",
      "          1       0.87      0.96      0.91      1917\n",
      "          2       0.80      0.58      0.67       416\n",
      "\n",
      "avg / total       0.84      0.85      0.83      2478\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.13      0.21       171\n",
      "          1       0.84      0.97      0.90      1894\n",
      "          2       0.80      0.50      0.62       413\n",
      "\n",
      "avg / total       0.82      0.83      0.81      2478\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.12      0.20       141\n",
      "          1       0.86      0.97      0.91      1930\n",
      "          2       0.82      0.56      0.66       407\n",
      "\n",
      "avg / total       0.84      0.86      0.83      2478\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.12      0.19       154\n",
      "          1       0.86      0.97      0.91      1915\n",
      "          2       0.79      0.55      0.65       409\n",
      "\n",
      "avg / total       0.82      0.84      0.82      2478\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.41      0.11      0.17       135\n",
      "          1       0.85      0.96      0.90      1907\n",
      "          2       0.80      0.55      0.65       436\n",
      "\n",
      "avg / total       0.82      0.84      0.82      2478\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.11      0.17       133\n",
      "          1       0.86      0.97      0.91      1937\n",
      "          2       0.79      0.55      0.65       408\n",
      "\n",
      "avg / total       0.83      0.85      0.83      2478\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.17      0.26       124\n",
      "          1       0.88      0.98      0.92      1942\n",
      "          2       0.86      0.58      0.70       412\n",
      "\n",
      "avg / total       0.86      0.87      0.85      2478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "C = [2e-3, 2e-2, 2e-1, 2e-0, 2e-1, 2e-2, 2e-3]\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "clf = LogisticRegression(multi_class='ovr',solver='newton-cg')\n",
    "pipeline = Pipeline([('scaler', std_scaler)  , ('clf', clf)])\n",
    "param = [{'clf__C': C}]\n",
    "\n",
    "logreg_err = nested_kfold_cv(pipeline, param, X, Y)\n",
    "\n",
    "\n",
    "\n",
    "#TODO: logicstic regression, model selection, kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logreg_err' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-768ce2fd6f0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mttest_rel\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpaired_t_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mt_stat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpaired_t_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogreg_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvm_err\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mp_val\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logreg_err' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_rel as paired_t_test\n",
    "\n",
    "t_stat, p_val = paired_t_test(logreg_err, svm_err)\n",
    "\n",
    "if p_val <= 0.05:\n",
    "    print('\\np-value = ', p_val, ', hipoteza se odbacuje.')\n",
    "else:\n",
    "    print('\\np-value = ', p_val, ', hipoteza se ne odbacuje.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10450</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I didn't forsake all other bitches for my wife...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15632</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @FunSizedYogi: @TheBlackVoice well how else...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14059</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Oreo Ice Cream Sandwich http://t.co/RM0KsY99Bc\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19843</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @kcarmouche: How you real, don't put NO mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9145</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Dwight could've had me instead of that ugly yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24776</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>niggas stay with insides over some hoe shit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>361</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>\"@kazooook: We actin like it's the 7th day @We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9497</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Fuck around wit me n my niggas n we'll turn di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>21391</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>So at work today I got called a cracker ass bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2789</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>@CDNBallJunkie Spurs bitch that's the team, yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8592</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Can't take u coons nowhere &amp;amp; I mean nowhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>21035</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>SHIT! RT @KingHov1313: I want a bitch so bad t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8640</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Chance is trash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5801</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@bonnoxxx nah she said you a fag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4430</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>@Nubianreine_ you women. And when you getting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9319</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Faux Noise pundits don't like duck molester, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>850</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>#faggots are MERELY a #scientific indicator th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>14005</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Only faggots use white ds's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14998</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @ChanseyDaRapper: Make my way right to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>22351</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>They should have never gave a cracker a transm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>25227</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>wondertrade is the best pokemon feature ever. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>14096</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Parents Television Council: 'Scandal' sex scen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5403</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>@ZP3_ the little girl Tom Sawyer gets trapped ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3274</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@FrankieJGrande omg gtfo white faggot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2821</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>@CallNeeshCakey no , a pop wop &amp;#128514;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2642</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@BeyondTomcat you'd like that wouldn't you fag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5538</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@_____0__o______ faggot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>12882</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Log off nigger RT @PoloKingBC: #relationshipgo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11994</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>If your pussy stink, you aint goin to heaven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>24804</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>now time to find the monkey term scripts!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4260</th>\n",
       "      <td>20649</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @tolkienproverb: Fairy tale does not deny t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4261</th>\n",
       "      <td>1346</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>&amp;#8220;@Iamjayla__: @WB_Pooh_3x Bol that calle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4262</th>\n",
       "      <td>23205</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>When people tailgate, then pass you up, and lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4263</th>\n",
       "      <td>24394</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>harm this pussy instead RT @ABC7: missing 26-y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4264</th>\n",
       "      <td>13446</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>My nips are cold &amp;#128563;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4265</th>\n",
       "      <td>9153</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ESPN keep hiring these dyke bitches and gay Sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4266</th>\n",
       "      <td>24923</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>shit i wanna know if my bitch gone protect me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4267</th>\n",
       "      <td>4874</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>@SoutheastQueen ignore sorry I called you a bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4268</th>\n",
       "      <td>25113</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>too many grown man out here worryin' about the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4269</th>\n",
       "      <td>19969</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @lildurk_: S/o all my real fans who can wai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4270</th>\n",
       "      <td>10044</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Hello American people, my name is Eric Cantor ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4271</th>\n",
       "      <td>5707</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@asshole_king oh no. They drafted that gay ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4272</th>\n",
       "      <td>971</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>&amp;#128075; hi-ho http://t.co/FiC4FnRutZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4273</th>\n",
       "      <td>11703</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>I'm the biggest redskins dam right now if they...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4274</th>\n",
       "      <td>3094</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@DevilFzyq @DevilElyie @TehDevilClan shut up fag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4275</th>\n",
       "      <td>11777</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>If Ashton is trash then what does that make me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4276</th>\n",
       "      <td>15451</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @EarlyLegend: I don't fuck with these hoes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4277</th>\n",
       "      <td>5218</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>@TylerCarboner hey I'll leave Abbey too! Oh if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4278</th>\n",
       "      <td>10124</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Hoes gon' be hoes, let them be.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4279</th>\n",
       "      <td>4962</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@THERACISTDOCTOR One less little nigger that w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4280</th>\n",
       "      <td>11341</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>I was informed about a bird (pheonix) today th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4281</th>\n",
       "      <td>16362</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @Kevineffinskaff: My idea of a diet is eati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4282</th>\n",
       "      <td>23195</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>When my friends are roastin my other friends a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4283</th>\n",
       "      <td>16380</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @KiingVon: These bitches so fucking stupid ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4284</th>\n",
       "      <td>23064</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>What a fucking faggot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4285</th>\n",
       "      <td>13106</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Maybe next time i day drink ill balance it out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4286</th>\n",
       "      <td>2504</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@AnimeKing420 fight me 1 on 1 ur choice of gam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4287</th>\n",
       "      <td>3153</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>@DriveMy_Lexis yasssss bitch yasssss &amp;#128525;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4288</th>\n",
       "      <td>20181</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @netflix: Here's your first look at Charlie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4289</th>\n",
       "      <td>4836</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@Shmalec13 ya fag</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4290 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0          10450      3            2                   1        0      0   \n",
       "1          15632      3            2                   0        1      0   \n",
       "2          14059      3            0                   0        3      2   \n",
       "3          19843      3            2                   1        0      0   \n",
       "4           9145      3            0                   3        0      1   \n",
       "5          24776      3            0                   3        0      1   \n",
       "6            361      3            0                   0        3      2   \n",
       "7           9497      3            1                   2        0      1   \n",
       "8          21391      3            0                   3        0      1   \n",
       "9           2789      9            0                   9        0      1   \n",
       "10          8592      6            4                   2        0      0   \n",
       "11         21035      6            1                   5        0      1   \n",
       "12          8640      3            0                   1        2      2   \n",
       "13          5801      3            2                   1        0      0   \n",
       "14          4430      3            0                   2        1      1   \n",
       "15          9319      3            2                   0        1      0   \n",
       "16           850      3            2                   0        1      0   \n",
       "17         14005      3            2                   1        0      0   \n",
       "18         14998      3            0                   0        3      2   \n",
       "19         22351      3            2                   1        0      0   \n",
       "20         25227      3            0                   0        3      2   \n",
       "21         14096      3            0                   1        2      2   \n",
       "22          5403      3            0                   0        3      2   \n",
       "23          3274      3            2                   1        0      0   \n",
       "24          2821      3            0                   0        3      2   \n",
       "25          2642      3            2                   1        0      0   \n",
       "26          5538      3            2                   1        0      0   \n",
       "27         12882      3            0                   1        2      2   \n",
       "28         11994      3            0                   3        0      1   \n",
       "29         24804      6            1                   0        5      2   \n",
       "...          ...    ...          ...                 ...      ...    ...   \n",
       "4260       20649      3            0                   0        3      2   \n",
       "4261        1346      3            0                   2        1      1   \n",
       "4262       23205      3            0                   3        0      1   \n",
       "4263       24394      3            2                   1        0      0   \n",
       "4264       13446      3            0                   1        2      2   \n",
       "4265        9153      3            3                   0        0      0   \n",
       "4266       24923      3            0                   3        0      1   \n",
       "4267        4874      3            0                   3        0      1   \n",
       "4268       25113      3            1                   2        0      1   \n",
       "4269       19969      3            0                   0        3      2   \n",
       "4270       10044      6            0                   1        5      2   \n",
       "4271        5707      3            2                   1        0      0   \n",
       "4272         971      3            0                   1        2      2   \n",
       "4273       11703      6            0                   0        6      2   \n",
       "4274        3094      3            2                   1        0      0   \n",
       "4275       11777      3            0                   0        3      2   \n",
       "4276       15451      6            4                   2        0      0   \n",
       "4277        5218      3            0                   0        3      2   \n",
       "4278       10124      3            0                   3        0      1   \n",
       "4279        4962      3            3                   0        0      0   \n",
       "4280       11341      3            0                   0        3      2   \n",
       "4281       16362      3            0                   0        3      2   \n",
       "4282       23195      3            1                   2        0      1   \n",
       "4283       16380      3            0                   3        0      1   \n",
       "4284       23064      3            2                   1        0      0   \n",
       "4285       13106      3            0                   2        1      1   \n",
       "4286        2504      3            2                   1        0      0   \n",
       "4287        3153      3            0                   3        0      1   \n",
       "4288       20181      3            0                   0        3      2   \n",
       "4289        4836      3            2                   1        0      0   \n",
       "\n",
       "                                                  tweet  \n",
       "0     I didn't forsake all other bitches for my wife...  \n",
       "1     RT @FunSizedYogi: @TheBlackVoice well how else...  \n",
       "2       Oreo Ice Cream Sandwich http://t.co/RM0KsY99Bc\"  \n",
       "3     RT @kcarmouche: How you real, don't put NO mon...  \n",
       "4     Dwight could've had me instead of that ugly yo...  \n",
       "5           niggas stay with insides over some hoe shit  \n",
       "6     \"@kazooook: We actin like it's the 7th day @We...  \n",
       "7     Fuck around wit me n my niggas n we'll turn di...  \n",
       "8     So at work today I got called a cracker ass bi...  \n",
       "9     @CDNBallJunkie Spurs bitch that's the team, yo...  \n",
       "10      Can't take u coons nowhere &amp; I mean nowhere  \n",
       "11    SHIT! RT @KingHov1313: I want a bitch so bad t...  \n",
       "12                                      Chance is trash  \n",
       "13                     @bonnoxxx nah she said you a fag  \n",
       "14    @Nubianreine_ you women. And when you getting ...  \n",
       "15    Faux Noise pundits don't like duck molester, p...  \n",
       "16    #faggots are MERELY a #scientific indicator th...  \n",
       "17                          Only faggots use white ds's  \n",
       "18    RT @ChanseyDaRapper: Make my way right to the ...  \n",
       "19    They should have never gave a cracker a transm...  \n",
       "20    wondertrade is the best pokemon feature ever. ...  \n",
       "21    Parents Television Council: 'Scandal' sex scen...  \n",
       "22    @ZP3_ the little girl Tom Sawyer gets trapped ...  \n",
       "23                @FrankieJGrande omg gtfo white faggot  \n",
       "24             @CallNeeshCakey no , a pop wop &#128514;  \n",
       "25       @BeyondTomcat you'd like that wouldn't you fag  \n",
       "26                              @_____0__o______ faggot  \n",
       "27    Log off nigger RT @PoloKingBC: #relationshipgo...  \n",
       "28         If your pussy stink, you aint goin to heaven  \n",
       "29            now time to find the monkey term scripts!  \n",
       "...                                                 ...  \n",
       "4260  RT @tolkienproverb: Fairy tale does not deny t...  \n",
       "4261  &#8220;@Iamjayla__: @WB_Pooh_3x Bol that calle...  \n",
       "4262  When people tailgate, then pass you up, and lo...  \n",
       "4263  harm this pussy instead RT @ABC7: missing 26-y...  \n",
       "4264                         My nips are cold &#128563;  \n",
       "4265  ESPN keep hiring these dyke bitches and gay Sp...  \n",
       "4266  shit i wanna know if my bitch gone protect me ...  \n",
       "4267  @SoutheastQueen ignore sorry I called you a bi...  \n",
       "4268  too many grown man out here worryin' about the...  \n",
       "4269  RT @lildurk_: S/o all my real fans who can wai...  \n",
       "4270  Hello American people, my name is Eric Cantor ...  \n",
       "4271  @asshole_king oh no. They drafted that gay ass...  \n",
       "4272             &#128075; hi-ho http://t.co/FiC4FnRutZ  \n",
       "4273  I'm the biggest redskins dam right now if they...  \n",
       "4274   @DevilFzyq @DevilElyie @TehDevilClan shut up fag  \n",
       "4275  If Ashton is trash then what does that make me...  \n",
       "4276  RT @EarlyLegend: I don't fuck with these hoes ...  \n",
       "4277  @TylerCarboner hey I'll leave Abbey too! Oh if...  \n",
       "4278                    Hoes gon' be hoes, let them be.  \n",
       "4279  @THERACISTDOCTOR One less little nigger that w...  \n",
       "4280  I was informed about a bird (pheonix) today th...  \n",
       "4281  RT @Kevineffinskaff: My idea of a diet is eati...  \n",
       "4282  When my friends are roastin my other friends a...  \n",
       "4283  RT @KiingVon: These bitches so fucking stupid ...  \n",
       "4284                              What a fucking faggot  \n",
       "4285  Maybe next time i day drink ill balance it out...  \n",
       "4286  @AnimeKing420 fight me 1 on 1 ur choice of gam...  \n",
       "4287     @DriveMy_Lexis yasssss bitch yasssss &#128525;  \n",
       "4288  RT @netflix: Here's your first look at Charlie...  \n",
       "4289                                  @Shmalec13 ya fag  \n",
       "\n",
       "[4290 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sampling data so we get equal distribution for all classes. used in active learning\n",
    "d0 = dataset[dataset['class'] == 0]\n",
    "d1 = dataset[dataset['class'] == 1].sample(d0.shape[0])\n",
    "d2 = dataset[dataset['class'] == 2].sample(d0.shape[0])\n",
    "\n",
    "#Dl data that is representing same number of all classes. From this we will take n data for starter training as\n",
    "#labeled data, and rest for unlabeled data for active learning\n",
    "Dl = pd.concat([d0,d1,d2])\n",
    "Dl = sklearn.utils.shuffle(Dl)\n",
    "Dl.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_filtered_tweets=[];\n",
    "AL_tweet_tags = [];\n",
    "AL_filtered_tweets_stemmed=[];\n",
    "AL_common_words_prepare=[];\n",
    "\n",
    "for tweet in Dl['tweet']:\n",
    "    tweet = clean_text([\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\".*@.*:\",\"&#*\\w*\",\"@[\\w\\-]+\",\"[^\\w\\s]\"],tweet)\n",
    "    tweet = tweet.lower()\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    common_words = get_common_words(word_tokens)\n",
    "    for word in common_words:\n",
    "        AL_common_words_prepare.append(word)\n",
    "    AL_filtered_tweets.append([word for word in word_tokens if not word in stopwords_list])\n",
    "    AL_filtered_tweets_stemmed.append([ps.stem(word) for word in word_tokens if not word in stopwords_list])\n",
    "    \n",
    "for tweet in AL_filtered_tweets:\n",
    "    AL_tweet_tags.append(nltk.pos_tag(tweet))\n",
    "\n",
    "# Izbacivanje frequent wordsa\n",
    "\n",
    "AL_common_words = get_common_words(AL_common_words_prepare)\n",
    "#for word in filtered_tweets[1:100]:\n",
    "#    print(word)\n",
    "AL_common = []\n",
    "for word in AL_common_words[1:50]:\n",
    "   AL_common.append(word[0][0])\n",
    "\n",
    "AL_filtered_tweets_no_common = [];\n",
    "AL_filtered_tweets_no_common_stemmed = [];\n",
    "for line in AL_filtered_tweets:\n",
    "    AL_filtered_tweets_no_common.append([word for word in line if not word in AL_common])\n",
    "    AL_filtered_tweets_no_common_stemmed.append([ps.stem(word) for word in line if not word in AL_common])\n",
    "#print(\"---------------------------\")\n",
    "#for word in filtered_tweets_no_common[1:100]:\n",
    "#    print(word)\n",
    "\n",
    "AL_ftss=[]\n",
    "for tweet in AL_filtered_tweets_no_common_stemmed:\n",
    "    AL_ftss.append(' '.join(tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_X = vectorizer.fit_transform(AL_ftss).toarray()\n",
    "AL_Y = Dl['class'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "Tk_X, Tu_X, Tk_y, _ = train_test_split(AL_X, AL_Y, random_state=42, test_size=0.8)\n",
    "\n",
    "n_labeled_points = math.ceil(len(AL_Y)*0.05)\n",
    "unlabeled_indices = np.arange(len(AL_Y))[n_labeled_points:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:51: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "from sklearn.semi_supervised import label_propagation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "max_iterations = 100\n",
    "for i in range(max_iterations):\n",
    "    if len(Tu_X) == 0 :\n",
    "        print(\"No unlabeled data to label.\")\n",
    "        break\n",
    "    al_model = label_propagation.LabelSpreading(gamma=0.25,max_iter=200)\n",
    "    y_train = np.copy(AL_Y)\n",
    "    y_train[unlabeled_indices] = -1\n",
    "    al_model.fit(AL_X,y_train)\n",
    "    predicted_labels = al_model.transduction_[unlabeled_indices]\n",
    "    true_labels = [AL_Y[i] for i in unlabeled_indices]\n",
    "    #if set(al_model.classes_).issubset(set(true_labels)) == False:\n",
    "    #    break\n",
    "    #cm = confusion_matrix(true_labels, predicted_labels,\n",
    "    #                      labels=al_model.classes_)\n",
    "    #print(\"Iteration %i %s\" % (i, 70 * \"_\"))\n",
    "    \n",
    "        \n",
    "    #print(\"Label Spreading model: %d labeled & %d unlabeled (%d total)\"\n",
    "    #      % (n_labeled_points, len(AL_Y) - n_labeled_points,\n",
    "    #         len(AL_Y)))\n",
    "\n",
    "    #print(classification_report(true_labels, predicted_labels))\n",
    "\n",
    "    #print(\"Confusion matrix\")\n",
    "    #print(cm)\n",
    "\n",
    "    # compute the entropies of transduced label distributions\n",
    "    pred_entropies = stats.distributions.entropy(\n",
    "        al_model.label_distributions_.T)\n",
    "\n",
    "    # select up to 5 digit examples that the classifier is most uncertain about\n",
    "    uncertainty_index = np.argsort(pred_entropies)[::-1]\n",
    "    uncertainty_index = uncertainty_index[\n",
    "        np.in1d(uncertainty_index, unlabeled_indices)][:50]\n",
    "\n",
    "    # keep track of indices that we get labels for\n",
    "    delete_indices = np.array([])\n",
    "    \n",
    "    for index, check_index in enumerate(uncertainty_index):\n",
    "        check = AL_ftss[check_index]\n",
    "\n",
    "        delete_index, = np.where(unlabeled_indices == check_index)\n",
    "        delete_indices = np.concatenate((delete_indices, delete_index))\n",
    "\n",
    "    unlabeled_indices = np.delete(unlabeled_indices, delete_indices)\n",
    "    n_labeled_points += len(uncertainty_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.08      0.21      0.12       164\n",
      "          1       0.79      0.62      0.70      1905\n",
      "          2       0.19      0.26      0.22       410\n",
      "\n",
      "avg / total       0.65      0.53      0.58      2479\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\semi_supervised\\label_propagation.py:201: RuntimeWarning: invalid value encountered in true_divide\n",
      "  probabilities /= normalizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.06      0.19      0.09       127\n",
      "          1       0.78      0.60      0.68      1927\n",
      "          2       0.20      0.27      0.23       425\n",
      "\n",
      "avg / total       0.64      0.52      0.57      2479\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.11      0.35      0.16       136\n",
      "          1       0.79      0.62      0.69      1916\n",
      "          2       0.21      0.26      0.23       427\n",
      "\n",
      "avg / total       0.65      0.54      0.58      2479\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.08      0.22      0.12       145\n",
      "          1       0.81      0.63      0.71      1917\n",
      "          2       0.22      0.32      0.26       416\n",
      "\n",
      "avg / total       0.67      0.55      0.60      2478\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.11      0.29      0.16       171\n",
      "          1       0.79      0.60      0.68      1894\n",
      "          2       0.20      0.27      0.23       413\n",
      "\n",
      "avg / total       0.64      0.53      0.57      2478\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.07      0.21      0.10       141\n",
      "          1       0.78      0.59      0.67      1930\n",
      "          2       0.19      0.28      0.23       407\n",
      "\n",
      "avg / total       0.65      0.51      0.57      2478\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.10      0.27      0.15       154\n",
      "          1       0.79      0.63      0.70      1915\n",
      "          2       0.20      0.26      0.22       409\n",
      "\n",
      "avg / total       0.65      0.55      0.59      2478\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.07      0.22      0.11       135\n",
      "          1       0.79      0.62      0.70      1907\n",
      "          2       0.21      0.28      0.24       436\n",
      "\n",
      "avg / total       0.65      0.54      0.58      2478\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.07      0.24      0.11       133\n",
      "          1       0.79      0.59      0.68      1937\n",
      "          2       0.19      0.28      0.23       408\n",
      "\n",
      "avg / total       0.66      0.52      0.57      2478\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.07      0.22      0.10       124\n",
      "          1       0.81      0.62      0.70      1942\n",
      "          2       0.20      0.27      0.23       412\n",
      "\n",
      "avg / total       0.67      0.54      0.59      2478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "al_err = []\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    \n",
    "# Outer loop\n",
    "for ind_train, ind_test in kfold.split(X):\n",
    "        \n",
    "    X_train, y_train, X_test, y_test = X[ind_train], Y[ind_train], X[ind_test], Y[ind_test]\n",
    "        \n",
    "    # Prediction based on the best selected params, the ones that minimize average error\n",
    "    h = al_model.predict(X_test)\n",
    "        \n",
    "    al_err.append(zero_one_loss(y_test, h))\n",
    "    print(classification_report( y_test, h ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mislavz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mislavz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mislavz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialization for HateSpeechRecognition\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import t\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_kfold_cv(clf, param_grid, X, y, k1=10, k2=3):\n",
    "    \n",
    "    err = []\n",
    "    kfold = KFold(n_splits=k1, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Outer loop\n",
    "    for ind_train, ind_test in kfold.split(X):\n",
    "        \n",
    "        X_train, y_train, X_test, y_test = X[ind_train], y[ind_train], X[ind_test], y[ind_test]\n",
    "        \n",
    "        # Inner loop\n",
    "        inn = GridSearchCV(clf,param_grid,n_jobs=5, cv=StratifiedKFold(n_splits=5, \n",
    "                                              random_state=42).split(X_train, y_train), \n",
    "                           verbose=2).fit(X_train, y_train)\n",
    "        \n",
    "        # Prediction based on the best selected params, the ones that minimize average error\n",
    "        h = inn.best_estimator_.fit(X_train, y_train).predict(X_test)\n",
    "        \n",
    "        err.append(zero_one_loss(y_test, h))\n",
    "        print(classification_report( y_test, h ))\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(rgx_list, text):\n",
    "    new_text = text\n",
    "    for rgx_match in rgx_list:\n",
    "        new_text = re.sub(rgx_match, '', new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_words(content):\n",
    "    ret_list = []\n",
    "    fdist2 = nltk.FreqDist(content)\n",
    "    most_list = fdist2.most_common(75)\n",
    "    for x in most_list:\n",
    "        ret_list.append(x)\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "#other_exclusions = [\"#ff\", \"ff\", \"rt\",\"!\",\":\",\"...\",\".\",\"-\",\"&\",\"?\"]\n",
    "#stopwords_list.extend(other_exclusions)\n",
    "\n",
    "dataset = pd.read_csv(\"dataset/labeled_data.csv\")\n",
    "\n",
    "tweets = dataset.tweet\n",
    "\n",
    "ps = nltk.stem.PorterStemmer()\n",
    "\n",
    "#ps.stem(dataset)\n",
    "\n",
    "#Text cleaning and tokenization, then stemming then POS tagging\n",
    "filtered_tweets=[];\n",
    "tweet_tags = [];\n",
    "filtered_tweets_stemmed=[];\n",
    "common_words_prepare=[];\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweet = clean_text([\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\".*@.*:\",\"&#*\\w*\",\"@[\\w\\-]+\",\"[^\\w\\s]\"],tweet)\n",
    "    tweet = tweet.lower()\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    common_words = get_common_words(word_tokens)\n",
    "    for word in common_words:\n",
    "        common_words_prepare.append(word)\n",
    "    filtered_tweets.append([word for word in word_tokens if not word in stopwords_list])\n",
    "    filtered_tweets_stemmed.append([ps.stem(word) for word in word_tokens if not word in stopwords_list])\n",
    "    \n",
    "for tweet in filtered_tweets:\n",
    "    tweet_tags.append(nltk.pos_tag(tweet))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Izbacivanje frequent wordsa\n",
    "\n",
    "common_words = get_common_words(common_words_prepare)\n",
    "#for word in filtered_tweets[1:100]:\n",
    "#    print(word)\n",
    "common = []\n",
    "for word in common_words[1:50]:\n",
    "   common.append(word[0][0])\n",
    "\n",
    "filtered_tweets_no_common = [];\n",
    "filtered_tweets_no_common_stemmed = [];\n",
    "for line in filtered_tweets:\n",
    "    filtered_tweets_no_common.append([word for word in line if not word in common])\n",
    "    filtered_tweets_no_common_stemmed.append([ps.stem(word) for word in line if not word in common])\n",
    "#print(\"---------------------------\")\n",
    "#for word in filtered_tweets_no_common[1:100]:\n",
    "#    print(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating TfIdf vectorizer\n",
    "ftss=[]\n",
    "for tweet in filtered_tweets_no_common_stemmed:\n",
    "    ftss.append(' '.join(tweet))\n",
    "\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(ngram_range=(1,3),lowercase=False,max_features=1000,smooth_idf=False,norm=None,max_df=0.75,min_df=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(ftss).toarray()\n",
    "Y = dataset['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=42, test_size=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 69.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.12      0.19       164\n",
      "          1       0.84      0.97      0.90      1905\n",
      "          2       0.79      0.44      0.56       410\n",
      "\n",
      "avg / total       0.80      0.82      0.80      2479\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 69.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.38      0.16      0.22       127\n",
      "          1       0.84      0.97      0.90      1927\n",
      "          2       0.82      0.42      0.56       425\n",
      "\n",
      "avg / total       0.82      0.83      0.81      2479\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 67.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.10      0.17       136\n",
      "          1       0.84      0.97      0.90      1916\n",
      "          2       0.81      0.45      0.58       427\n",
      "\n",
      "avg / total       0.82      0.84      0.81      2479\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 69.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.14      0.21       145\n",
      "          1       0.85      0.97      0.90      1917\n",
      "          2       0.79      0.48      0.59       416\n",
      "\n",
      "avg / total       0.82      0.84      0.81      2478\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 70.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.16      0.26       171\n",
      "          1       0.83      0.98      0.90      1894\n",
      "          2       0.83      0.41      0.55       413\n",
      "\n",
      "avg / total       0.83      0.83      0.80      2478\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 67.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.15      0.23       141\n",
      "          1       0.85      0.97      0.91      1930\n",
      "          2       0.85      0.49      0.62       407\n",
      "\n",
      "avg / total       0.83      0.85      0.82      2478\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 69.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.16      0.25       154\n",
      "          1       0.84      0.97      0.90      1915\n",
      "          2       0.80      0.45      0.57       409\n",
      "\n",
      "avg / total       0.82      0.83      0.81      2478\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 66.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.13      0.21       135\n",
      "          1       0.84      0.97      0.90      1907\n",
      "          2       0.82      0.45      0.58       436\n",
      "\n",
      "avg / total       0.82      0.83      0.81      2478\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 72.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.13      0.21       133\n",
      "          1       0.86      0.97      0.91      1937\n",
      "          2       0.81      0.49      0.61       408\n",
      "\n",
      "avg / total       0.84      0.85      0.82      2478\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 68.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.42      0.13      0.20       124\n",
      "          1       0.86      0.98      0.91      1942\n",
      "          2       0.86      0.50      0.63       412\n",
      "\n",
      "avg / total       0.84      0.85      0.83      2478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SVM Classifier\n",
    "C = [ 2e-2, 2e-1, 2e-0]\n",
    "gama = [ 2e-2, 2e-1, 2e-0]\n",
    "param = [{'svc__kernel': ['rbf'], 'svc__C': C}]\n",
    "clf = svm.SVC(decision_function_shape='ovo')\n",
    "scale = StandardScaler()\n",
    "pipeline = Pipeline([('scaler', scale), ('svc', clf)])\n",
    "\n",
    "svm_err = nested_kfold_cv(pipeline, param, X, Y)\n",
    "#param_grid = [{}] \n",
    "#grid_search = GridSearchCV(pipeline, \n",
    "#                           param_grid,\n",
    "#                          n_jobs = 5,\n",
    "#                           cv=StratifiedKFold(n_splits=5, \n",
    "#                                              random_state=42).split(X_train, y_train), \n",
    "#                           verbose=2)\n",
    "#model = grid_search.fit(X_train,y_train)\n",
    "#y_pred = model.predict(X_test)\n",
    "#report = classification_report( y_test, y_pred )\n",
    "#print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.10      0.17       164\n",
      "          1       0.85      0.97      0.90      1905\n",
      "          2       0.80      0.54      0.65       410\n",
      "\n",
      "avg / total       0.82      0.84      0.81      2479\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.47      0.12      0.19       127\n",
      "          1       0.86      0.97      0.91      1927\n",
      "          2       0.80      0.50      0.62       425\n",
      "\n",
      "avg / total       0.83      0.85      0.82      2479\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.10      0.17       136\n",
      "          1       0.86      0.97      0.91      1916\n",
      "          2       0.81      0.55      0.66       427\n",
      "\n",
      "avg / total       0.83      0.85      0.83      2479\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.15      0.24       145\n",
      "          1       0.87      0.96      0.91      1917\n",
      "          2       0.80      0.58      0.67       416\n",
      "\n",
      "avg / total       0.84      0.85      0.83      2478\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.13      0.21       171\n",
      "          1       0.84      0.97      0.90      1894\n",
      "          2       0.80      0.50      0.62       413\n",
      "\n",
      "avg / total       0.82      0.83      0.81      2478\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.12      0.20       141\n",
      "          1       0.86      0.97      0.91      1930\n",
      "          2       0.82      0.56      0.66       407\n",
      "\n",
      "avg / total       0.84      0.86      0.83      2478\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.12      0.19       154\n",
      "          1       0.86      0.97      0.91      1915\n",
      "          2       0.79      0.55      0.65       409\n",
      "\n",
      "avg / total       0.82      0.84      0.82      2478\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.41      0.11      0.17       135\n",
      "          1       0.85      0.96      0.90      1907\n",
      "          2       0.80      0.55      0.65       436\n",
      "\n",
      "avg / total       0.82      0.84      0.82      2478\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.11      0.17       133\n",
      "          1       0.86      0.97      0.91      1937\n",
      "          2       0.79      0.55      0.65       408\n",
      "\n",
      "avg / total       0.83      0.85      0.83      2478\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.17      0.26       124\n",
      "          1       0.88      0.98      0.92      1942\n",
      "          2       0.86      0.58      0.70       412\n",
      "\n",
      "avg / total       0.86      0.87      0.85      2478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "C = [2e-3, 2e-2, 2e-1, 2e-0, 2e-1, 2e-2, 2e-3]\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "clf = LogisticRegression(multi_class='ovr',solver='newton-cg')\n",
    "pipeline = Pipeline([('scaler', std_scaler)  , ('clf', clf)])\n",
    "param = [{'clf__C': C}]\n",
    "\n",
    "logreg_err = nested_kfold_cv(pipeline, param, X, Y)\n",
    "\n",
    "\n",
    "\n",
    "#TODO: logicstic regression, model selection, kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logreg_err' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-768ce2fd6f0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mttest_rel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpaired_t_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mt_stat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaired_t_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogreg_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvm_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp_val\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logreg_err' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_rel as paired_t_test\n",
    "\n",
    "t_stat, p_val = paired_t_test(logreg_err, svm_err)\n",
    "\n",
    "if p_val <= 0.05:\n",
    "    print('\\np-value = ', p_val, ', hipoteza se odbacuje.')\n",
    "else:\n",
    "    print('\\np-value = ', p_val, ', hipoteza se ne odbacuje.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1230</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&amp;#8220;@Crowdifornia: We agree... do you? http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11146</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I say K.Michelle is manly because she doesn't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4348</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>@NYRKelsMads \\nDat damn dog eats betta den mos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13354</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>My eyes feel so chink eyed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3709</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@JasonMoriarty13 I remember his nephew Lucas c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17021</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @NickCoyne001: Guys who tweet shit to get a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23060</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>What a Dirty Girl! Haha &amp;#58381;&amp;#57423;&amp;#5838...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>190</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>\"@MaxMayo77: http://t.co/XpSC3makgJ\" sexy lad ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14404</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @AChosenWord: Of rain showers\\n&amp;amp; weepy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9355</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Find a big butt bitch somewhere and get my nut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14501</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @AlxJames10: I mean sometimes eating a cart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7007</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@passIt2Liv Hit a bitch it's not like you can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12858</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Lmfao RT @NYDailyNews: The New York @Yankees h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>23941</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>You&amp;#8217;re a little bitch RT @FuckingCashBro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9240</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Every spic cop should be killed. Those pigs on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3273</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@FrankieJGrande fugly queer white trash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2644</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@Bidenshairplugs He looks like a fag and his t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>12690</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Lethal bird flu cocktail sent out of lab accid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>12378</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Jihadis keep on prattling about having shot do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10999</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>I love jigg jigg&amp;#128514;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>12036</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Im looking for a reason to really spazz on a b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5467</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>@_IAmDriven_ @HDRealtor not even sure bro, Im ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>13548</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Never gas hoes but I do take they ass home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>18896</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @_prettygirl92: bitches jus love to get me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10967</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I look like Denzel when I tilt my head to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7974</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>And the Jew @Deuterostomes favorited my tweet ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>23490</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Wussup pussies ;) #ImBack #NewTwitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>23704</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>You a hoe if you like gettin drunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>22762</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Turn on the magic of colored light. #longexpos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>18691</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @_HolySmokes: If you can't eat pussy that's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4260</th>\n",
       "      <td>17750</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @SnipersInPeril: @ShinSnipes happy birthday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4261</th>\n",
       "      <td>15452</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @EarthPix: Up with the birds to fish on the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4262</th>\n",
       "      <td>3362</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>@GoHard_Brown @your_daddy9 &amp;amp; xavier you bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4263</th>\n",
       "      <td>11690</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I'm starting to not like meek, this nig a lil ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4264</th>\n",
       "      <td>9677</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Getting hit off a cunt thats about 6 foot side...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4265</th>\n",
       "      <td>4351</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>@NYRKelsMads \\nI gots to be bein colored and all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4266</th>\n",
       "      <td>8670</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Charlie bit me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4267</th>\n",
       "      <td>15783</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @HG_Shit: Mfs still in the same spot as 3yr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4268</th>\n",
       "      <td>7947</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>And even if the sky come fallin, bitch ima sti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4269</th>\n",
       "      <td>6136</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>@fennecuskitsune you've got no sole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4270</th>\n",
       "      <td>16981</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @Nachumlist: Charlie Rangel: &amp;#180;The Thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4271</th>\n",
       "      <td>11270</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I told that bitch IDGAF about a Benz, bitch!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4272</th>\n",
       "      <td>5144</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>@Threehoez rip plate, brownies, large tea, fri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4273</th>\n",
       "      <td>2733</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@BreeOlson @Gingerbeard66 your a nasty nigger ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4274</th>\n",
       "      <td>14009</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Only fw bad bitches iont do no mediocre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4275</th>\n",
       "      <td>9310</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Fat ass hoe holding up the machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4276</th>\n",
       "      <td>4246</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@Mimistheone You really seem to love Sandusky....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4277</th>\n",
       "      <td>6580</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@kiara_curry i know im so kidding. Some of BET...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4278</th>\n",
       "      <td>6177</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@gayle_ryan I don't hate the Japs. But, I AINT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4279</th>\n",
       "      <td>22604</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>This re-colored pictured of Abraham Lincoln is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4280</th>\n",
       "      <td>22651</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Thw White Iron Band plays this weekend in Farg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4281</th>\n",
       "      <td>23320</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Why 2 dudes would wanna fuck the same bitch at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4282</th>\n",
       "      <td>1474</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&amp;#8220;@NoChillPaz: White kids favorite activi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4283</th>\n",
       "      <td>3687</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@JacobbBacker fag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4284</th>\n",
       "      <td>17838</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @SteveStfler: Rise of the planet of the ape...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4285</th>\n",
       "      <td>22851</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Uh oh RT @VICD713ENT: Control yo bitch if you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4286</th>\n",
       "      <td>24690</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>lol unfaithful hoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4287</th>\n",
       "      <td>3496</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@HuffingtonPost if the faggots can have a page...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4288</th>\n",
       "      <td>24894</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>redneck in the White House // Moccasin Creek &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4289</th>\n",
       "      <td>8925</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>DeeDee, Patrick, Ed, Sheen, Timmy's Dad, Beavi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4290 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           1230      3            2                   1        0      0   \n",
       "1          11146      3            1                   2        0      1   \n",
       "2           4348      3            0                   1        2      2   \n",
       "3          13354      6            4                   2        0      0   \n",
       "4           3709      3            2                   0        1      0   \n",
       "5          17021      3            0                   3        0      1   \n",
       "6          23060      3            0                   3        0      1   \n",
       "7            190      3            0                   1        2      2   \n",
       "8          14404      3            0                   0        3      2   \n",
       "9           9355      3            0                   3        0      1   \n",
       "10         14501      3            0                   0        3      2   \n",
       "11          7007      3            2                   1        0      0   \n",
       "12         12858      3            0                   0        3      2   \n",
       "13         23941      3            2                   1        0      0   \n",
       "14          9240      3            3                   0        0      0   \n",
       "15          3273      3            3                   0        0      0   \n",
       "16          2644      3            2                   1        0      0   \n",
       "17         12690      3            0                   1        2      2   \n",
       "18         12378      3            0                   0        3      2   \n",
       "19         10999      3            0                   0        3      2   \n",
       "20         12036      3            0                   3        0      1   \n",
       "21          5467      3            0                   3        0      1   \n",
       "22         13548      3            0                   3        0      1   \n",
       "23         18896      3            0                   3        0      1   \n",
       "24         10967      3            1                   2        0      1   \n",
       "25          7974      3            2                   1        0      0   \n",
       "26         23490      3            0                   3        0      1   \n",
       "27         23704      3            1                   2        0      1   \n",
       "28         22762      3            0                   0        3      2   \n",
       "29         18691      3            0                   3        0      1   \n",
       "...          ...    ...          ...                 ...      ...    ...   \n",
       "4260       17750      3            2                   1        0      0   \n",
       "4261       15452      3            0                   0        3      2   \n",
       "4262        3362      3            0                   3        0      1   \n",
       "4263       11690      3            2                   1        0      0   \n",
       "4264        9677      6            3                   2        1      0   \n",
       "4265        4351      3            0                   0        3      2   \n",
       "4266        8670      3            0                   0        3      2   \n",
       "4267       15783      3            2                   1        0      0   \n",
       "4268        7947      3            0                   3        0      1   \n",
       "4269        6136      6            0                   1        5      2   \n",
       "4270       16981      3            0                   0        3      2   \n",
       "4271       11270      3            1                   2        0      1   \n",
       "4272        5144      3            0                   1        2      2   \n",
       "4273        2733      3            2                   1        0      0   \n",
       "4274       14009      3            0                   3        0      1   \n",
       "4275        9310      3            2                   1        0      0   \n",
       "4276        4246      3            2                   1        0      0   \n",
       "4277        6580      3            2                   1        0      0   \n",
       "4278        6177      3            2                   1        0      0   \n",
       "4279       22604      3            0                   1        2      2   \n",
       "4280       22651      3            0                   0        3      2   \n",
       "4281       23320      3            0                   3        0      1   \n",
       "4282        1474      3            3                   0        0      0   \n",
       "4283        3687      3            2                   1        0      0   \n",
       "4284       17838      3            0                   0        3      2   \n",
       "4285       22851      3            1                   2        0      1   \n",
       "4286       24690      3            0                   3        0      1   \n",
       "4287        3496      3            3                   0        0      0   \n",
       "4288       24894      3            2                   0        1      0   \n",
       "4289        8925      3            2                   1        0      0   \n",
       "\n",
       "                                                  tweet  \n",
       "0     &#8220;@Crowdifornia: We agree... do you? http...  \n",
       "1     I say K.Michelle is manly because she doesn't ...  \n",
       "2     @NYRKelsMads \\nDat damn dog eats betta den mos...  \n",
       "3                            My eyes feel so chink eyed  \n",
       "4     @JasonMoriarty13 I remember his nephew Lucas c...  \n",
       "5     RT @NickCoyne001: Guys who tweet shit to get a...  \n",
       "6     What a Dirty Girl! Haha &#58381;&#57423;&#5838...  \n",
       "7     \"@MaxMayo77: http://t.co/XpSC3makgJ\" sexy lad ...  \n",
       "8     RT @AChosenWord: Of rain showers\\n&amp; weepy ...  \n",
       "9     Find a big butt bitch somewhere and get my nut...  \n",
       "10    RT @AlxJames10: I mean sometimes eating a cart...  \n",
       "11    @passIt2Liv Hit a bitch it's not like you can ...  \n",
       "12    Lmfao RT @NYDailyNews: The New York @Yankees h...  \n",
       "13    You&#8217;re a little bitch RT @FuckingCashBro...  \n",
       "14    Every spic cop should be killed. Those pigs on...  \n",
       "15              @FrankieJGrande fugly queer white trash  \n",
       "16    @Bidenshairplugs He looks like a fag and his t...  \n",
       "17    Lethal bird flu cocktail sent out of lab accid...  \n",
       "18    Jihadis keep on prattling about having shot do...  \n",
       "19                            I love jigg jigg&#128514;  \n",
       "20    Im looking for a reason to really spazz on a b...  \n",
       "21    @_IAmDriven_ @HDRealtor not even sure bro, Im ...  \n",
       "22           Never gas hoes but I do take they ass home  \n",
       "23    RT @_prettygirl92: bitches jus love to get me ...  \n",
       "24    I look like Denzel when I tilt my head to the ...  \n",
       "25    And the Jew @Deuterostomes favorited my tweet ...  \n",
       "26                Wussup pussies ;) #ImBack #NewTwitter  \n",
       "27                   You a hoe if you like gettin drunk  \n",
       "28    Turn on the magic of colored light. #longexpos...  \n",
       "29    RT @_HolySmokes: If you can't eat pussy that's...  \n",
       "...                                                 ...  \n",
       "4260  RT @SnipersInPeril: @ShinSnipes happy birthday...  \n",
       "4261  RT @EarthPix: Up with the birds to fish on the...  \n",
       "4262  @GoHard_Brown @your_daddy9 &amp; xavier you bi...  \n",
       "4263  I'm starting to not like meek, this nig a lil ...  \n",
       "4264  Getting hit off a cunt thats about 6 foot side...  \n",
       "4265   @NYRKelsMads \\nI gots to be bein colored and all  \n",
       "4266                                     Charlie bit me  \n",
       "4267  RT @HG_Shit: Mfs still in the same spot as 3yr...  \n",
       "4268  And even if the sky come fallin, bitch ima sti...  \n",
       "4269                @fennecuskitsune you've got no sole  \n",
       "4270  RT @Nachumlist: Charlie Rangel: &#180;The Thin...  \n",
       "4271       I told that bitch IDGAF about a Benz, bitch!  \n",
       "4272  @Threehoez rip plate, brownies, large tea, fri...  \n",
       "4273  @BreeOlson @Gingerbeard66 your a nasty nigger ...  \n",
       "4274         Only fw bad bitches iont do no mediocre...  \n",
       "4275                 Fat ass hoe holding up the machine  \n",
       "4276  @Mimistheone You really seem to love Sandusky....  \n",
       "4277  @kiara_curry i know im so kidding. Some of BET...  \n",
       "4278  @gayle_ryan I don't hate the Japs. But, I AINT...  \n",
       "4279  This re-colored pictured of Abraham Lincoln is...  \n",
       "4280  Thw White Iron Band plays this weekend in Farg...  \n",
       "4281  Why 2 dudes would wanna fuck the same bitch at...  \n",
       "4282  &#8220;@NoChillPaz: White kids favorite activi...  \n",
       "4283                                  @JacobbBacker fag  \n",
       "4284  RT @SteveStfler: Rise of the planet of the ape...  \n",
       "4285  Uh oh RT @VICD713ENT: Control yo bitch if you ...  \n",
       "4286                                lol unfaithful hoes  \n",
       "4287  @HuffingtonPost if the faggots can have a page...  \n",
       "4288  redneck in the White House // Moccasin Creek &...  \n",
       "4289  DeeDee, Patrick, Ed, Sheen, Timmy's Dad, Beavi...  \n",
       "\n",
       "[4290 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sampling data so we get equal distribution for all classes. used in active learning\n",
    "d0 = dataset[dataset['class'] == 0]\n",
    "d1 = dataset[dataset['class'] == 1].sample(d0.shape[0])\n",
    "d2 = dataset[dataset['class'] == 2].sample(d0.shape[0])\n",
    "\n",
    "#Dl data that is representing same number of all classes. From this we will take n data for starter training as\n",
    "#labeled data, and rest for unlabeled data for active learning\n",
    "Dl = pd.concat([d0,d1,d2])\n",
    "Dl = sklearn.utils.shuffle(Dl)\n",
    "Dl.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_filtered_tweets=[];\n",
    "AL_tweet_tags = [];\n",
    "AL_filtered_tweets_stemmed=[];\n",
    "AL_common_words_prepare=[];\n",
    "\n",
    "for tweet in Dl['tweet']:\n",
    "    tweet = clean_text([\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\".*@.*:\",\"&#*\\w*\",\"@[\\w\\-]+\",\"[^\\w\\s]\"],tweet)\n",
    "    tweet = tweet.lower()\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    common_words = get_common_words(word_tokens)\n",
    "    for word in common_words:\n",
    "        AL_common_words_prepare.append(word)\n",
    "    AL_filtered_tweets.append([word for word in word_tokens if not word in stopwords_list])\n",
    "    AL_filtered_tweets_stemmed.append([ps.stem(word) for word in word_tokens if not word in stopwords_list])\n",
    "    \n",
    "for tweet in AL_filtered_tweets:\n",
    "    AL_tweet_tags.append(nltk.pos_tag(tweet))\n",
    "\n",
    "# Izbacivanje frequent wordsa\n",
    "\n",
    "AL_common_words = get_common_words(AL_common_words_prepare)\n",
    "#for word in filtered_tweets[1:100]:\n",
    "#    print(word)\n",
    "AL_common = []\n",
    "for word in AL_common_words[1:50]:\n",
    "   AL_common.append(word[0][0])\n",
    "\n",
    "AL_filtered_tweets_no_common = [];\n",
    "AL_filtered_tweets_no_common_stemmed = [];\n",
    "for line in AL_filtered_tweets:\n",
    "    AL_filtered_tweets_no_common.append([word for word in line if not word in AL_common])\n",
    "    AL_filtered_tweets_no_common_stemmed.append([ps.stem(word) for word in line if not word in AL_common])\n",
    "#print(\"---------------------------\")\n",
    "#for word in filtered_tweets_no_common[1:100]:\n",
    "#    print(word)\n",
    "\n",
    "AL_ftss=[]\n",
    "for tweet in AL_filtered_tweets_no_common_stemmed:\n",
    "    AL_ftss.append(' '.join(tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_X = vectorizer.fit_transform(AL_ftss).toarray()\n",
    "AL_Y = Dl['class'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "Tk_X, Tu_X, Tk_y, _ = train_test_split(AL_X, AL_Y, random_state=42, test_size=0.8)\n",
    "\n",
    "n_labeled_points = math.ceil(len(AL_Y)*0.05)\n",
    "unlabeled_indices = np.arange(len(AL_Y))[n_labeled_points:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:51: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "from sklearn.semi_supervised import label_propagation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "max_iterations = 100\n",
    "for i in range(max_iterations):\n",
    "    if len(Tu_X) == 0 :\n",
    "        print(\"No unlabeled data to label.\")\n",
    "        break\n",
    "    al_model = label_propagation.LabelSpreading(gamma=0.25,max_iter=200)\n",
    "    y_train = np.copy(AL_Y)\n",
    "    y_train[unlabeled_indices] = -1\n",
    "    al_model.fit(AL_X,y_train)\n",
    "    predicted_labels = al_model.transduction_[unlabeled_indices]\n",
    "    true_labels = [AL_Y[i] for i in unlabeled_indices]\n",
    "    #if set(al_model.classes_).issubset(set(true_labels)) == False:\n",
    "    #    break\n",
    "    #cm = confusion_matrix(true_labels, predicted_labels,\n",
    "    #                      labels=al_model.classes_)\n",
    "    #print(\"Iteration %i %s\" % (i, 70 * \"_\"))\n",
    "    \n",
    "        \n",
    "    #print(\"Label Spreading model: %d labeled & %d unlabeled (%d total)\"\n",
    "    #      % (n_labeled_points, len(AL_Y) - n_labeled_points,\n",
    "    #         len(AL_Y)))\n",
    "\n",
    "    #print(classification_report(true_labels, predicted_labels))\n",
    "\n",
    "    #print(\"Confusion matrix\")\n",
    "    #print(cm)\n",
    "\n",
    "    # compute the entropies of transduced label distributions\n",
    "    pred_entropies = stats.distributions.entropy(\n",
    "        al_model.label_distributions_.T)\n",
    "\n",
    "    # select up to 5 digit examples that the classifier is most uncertain about\n",
    "    uncertainty_index = np.argsort(pred_entropies)[::-1]\n",
    "    uncertainty_index = uncertainty_index[\n",
    "        np.in1d(uncertainty_index, unlabeled_indices)][:50]\n",
    "\n",
    "    # keep track of indices that we get labels for\n",
    "    delete_indices = np.array([])\n",
    "    \n",
    "    for index, check_index in enumerate(uncertainty_index):\n",
    "        check = AL_ftss[check_index]\n",
    "\n",
    "        delete_index, = np.where(unlabeled_indices == check_index)\n",
    "        delete_indices = np.concatenate((delete_indices, delete_index))\n",
    "\n",
    "    unlabeled_indices = np.delete(unlabeled_indices, delete_indices)\n",
    "    n_labeled_points += len(uncertainty_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\semi_supervised\\label_propagation.py:293: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.label_distributions_ /= normalizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Spreading model: 1116 labeled & 21188 unlabeled (22304 total)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:41: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Spreading model: 1366 labeled & 20938 unlabeled (22304 total)\n",
      "Label Spreading model: 1616 labeled & 20688 unlabeled (22304 total)\n",
      "Label Spreading model: 1866 labeled & 20438 unlabeled (22304 total)\n",
      "Label Spreading model: 2116 labeled & 20188 unlabeled (22304 total)\n",
      "Label Spreading model: 2366 labeled & 19938 unlabeled (22304 total)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-8e374ff49b5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0myal_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0myal_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0munlabeled_indices\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mal_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myal_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\semi_supervised\\label_propagation.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[0ml_previous\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_distributions_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m             self.label_distributions_ = safe_sparse_dot(\n\u001b[1;32m--> 272\u001b[1;33m                 graph_matrix, self.label_distributions_)\n\u001b[0m\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'propagation'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "al_err = []\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    \n",
    "# Outer loop\n",
    "for ind_train, ind_test in kfold.split(X):\n",
    "        \n",
    "    X_train, y_train, X_test, y_test = X[ind_train], Y[ind_train], X[ind_test], Y[ind_test]\n",
    "    \n",
    "    n_labeled_points = math.ceil(len(y_train)*0.05)\n",
    "    unlabeled_indices = np.arange(len(y_train))[n_labeled_points:]\n",
    "    \n",
    "    max_iterations = 100\n",
    "    for i in range(max_iterations):\n",
    "        if len(Tu_X) == 0 :\n",
    "            print(\"No unlabeled data to label.\")\n",
    "            break\n",
    "        al_model = label_propagation.LabelSpreading(gamma=0.25,max_iter=200)\n",
    "        yal_train = np.copy(y_train)\n",
    "        yal_train[unlabeled_indices] = -1\n",
    "        al_model.fit(X_train,yal_train)\n",
    "    \n",
    "        if(i%5==0):\n",
    "            print(\"Label Spreading model: %d labeled & %d unlabeled (%d total)\"\n",
    "            % (n_labeled_points, len(y_train) - n_labeled_points,\n",
    "             len(y_train)))\n",
    "            h = al_model.predict(X_test)\n",
    "    \n",
    "            al_err.append(zero_one_loss(y_test, h))\n",
    "            print(classification_report( y_test, h ))\n",
    "        \n",
    "        pred_entropies = stats.distributions.entropy(al_model.label_distributions_.T)   \n",
    "        uncertainty_index = np.argsort(pred_entropies)[::-1]\n",
    "        uncertainty_index = uncertainty_index[np.in1d(uncertainty_index, unlabeled_indices)][:50]\n",
    "        delete_indices = np.array([])\n",
    "    \n",
    "        for index, check_index in enumerate(uncertainty_index):\n",
    "            check = X[check_index]\n",
    "\n",
    "            delete_index, = np.where(unlabeled_indices == check_index)\n",
    "            delete_indices = np.concatenate((delete_indices, delete_index))\n",
    "\n",
    "        unlabeled_indices = np.delete(unlabeled_indices, delete_indices)\n",
    "        n_labeled_points += len(uncertainty_index)\n",
    "    \n",
    "    #h = al_model.predict(X_test)\n",
    "    \n",
    "    al_err.append(zero_one_loss(y_test, h))\n",
    "    \n",
    "    #print(classification_report( y_test, h ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

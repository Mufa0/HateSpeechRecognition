{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mufa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/mufa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/mufa/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialization for HateSpeechRecognition\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import t\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_kfold_cv(clf, param_grid, X, y, k1=10, k2=3):\n",
    "    \n",
    "    err = []\n",
    "    kfold = KFold(n_splits=k1, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Outer loop\n",
    "    for ind_train, ind_test in kfold.split(X):\n",
    "        \n",
    "        X_train, y_train, X_test, y_test = X[ind_train], y[ind_train], X[ind_test], y[ind_test]\n",
    "        \n",
    "        # Inner loop\n",
    "        inn = GridSearchCV(clf,param_grid,n_jobs=5, cv=StratifiedKFold(n_splits=5, \n",
    "                                              random_state=42).split(X_train, y_train), \n",
    "                           verbose=2).fit(X_train, y_train)\n",
    "        \n",
    "        # Prediction based on the best selected params, the ones that minimize average error\n",
    "        h = inn.best_estimator_.fit(X_train, y_train).predict(X_test)\n",
    "        \n",
    "        err.append(zero_one_loss(y_test, h))\n",
    "        print(classification_report( y_test, h ))\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(rgx_list, text):\n",
    "    new_text = text\n",
    "    for rgx_match in rgx_list:\n",
    "        new_text = re.sub(rgx_match, '', new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_words(content):\n",
    "    ret_list = []\n",
    "    fdist2 = nltk.FreqDist(content)\n",
    "    most_list = fdist2.most_common(75)\n",
    "    for x in most_list:\n",
    "        ret_list.append(x)\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "#other_exclusions = [\"#ff\", \"ff\", \"rt\",\"!\",\":\",\"...\",\".\",\"-\",\"&\",\"?\"]\n",
    "#stopwords_list.extend(other_exclusions)\n",
    "\n",
    "dataset = pd.read_csv(\"dataset/labeled_data.csv\")\n",
    "\n",
    "tweets = dataset.tweet\n",
    "\n",
    "ps = nltk.stem.PorterStemmer()\n",
    "\n",
    "#ps.stem(dataset)\n",
    "\n",
    "#Text cleaning and tokenization, then stemming then POS tagging\n",
    "filtered_tweets=[];\n",
    "tweet_tags = [];\n",
    "filtered_tweets_stemmed=[];\n",
    "common_words_prepare=[];\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweet = clean_text([\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\".*@.*:\",\"&#*\\w*\",\"@[\\w\\-]+\",\"[^\\w\\s]\"],tweet)\n",
    "    tweet = tweet.lower()\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    common_words = get_common_words(word_tokens)\n",
    "    for word in common_words:\n",
    "        common_words_prepare.append(word)\n",
    "    filtered_tweets.append([word for word in word_tokens if not word in stopwords_list])\n",
    "    filtered_tweets_stemmed.append([ps.stem(word) for word in word_tokens if not word in stopwords_list])\n",
    "    \n",
    "for tweet in filtered_tweets:\n",
    "    tweet_tags.append(nltk.pos_tag(tweet))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Izbacivanje frequent wordsa\n",
    "\n",
    "common_words = get_common_words(common_words_prepare)\n",
    "#for word in filtered_tweets[1:100]:\n",
    "#    print(word)\n",
    "common = []\n",
    "for word in common_words[1:50]:\n",
    "   common.append(word[0][0])\n",
    "\n",
    "filtered_tweets_no_common = [];\n",
    "filtered_tweets_no_common_stemmed = [];\n",
    "for line in filtered_tweets:\n",
    "    filtered_tweets_no_common.append([word for word in line if not word in common])\n",
    "    filtered_tweets_no_common_stemmed.append([ps.stem(word) for word in line if not word in common])\n",
    "#print(\"---------------------------\")\n",
    "#for word in filtered_tweets_no_common[1:100]:\n",
    "#    print(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating TfIdf vectorizer\n",
    "ftss=[]\n",
    "for tweet in filtered_tweets_no_common_stemmed:\n",
    "    ftss.append(' '.join(tweet))\n",
    "\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(ngram_range=(1,3),lowercase=False,max_features=1000,smooth_idf=False,norm=None,max_df=0.75,min_df=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(ftss).toarray()\n",
    "Y = dataset['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=42, test_size=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 69.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.12      0.19       164\n",
      "          1       0.84      0.97      0.90      1905\n",
      "          2       0.79      0.44      0.56       410\n",
      "\n",
      "avg / total       0.80      0.82      0.80      2479\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 69.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.38      0.16      0.22       127\n",
      "          1       0.84      0.97      0.90      1927\n",
      "          2       0.82      0.42      0.56       425\n",
      "\n",
      "avg / total       0.82      0.83      0.81      2479\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 67.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.10      0.17       136\n",
      "          1       0.84      0.97      0.90      1916\n",
      "          2       0.81      0.45      0.58       427\n",
      "\n",
      "avg / total       0.82      0.84      0.81      2479\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 69.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.14      0.21       145\n",
      "          1       0.85      0.97      0.90      1917\n",
      "          2       0.79      0.48      0.59       416\n",
      "\n",
      "avg / total       0.82      0.84      0.81      2478\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 70.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.16      0.26       171\n",
      "          1       0.83      0.98      0.90      1894\n",
      "          2       0.83      0.41      0.55       413\n",
      "\n",
      "avg / total       0.83      0.83      0.80      2478\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 67.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.15      0.23       141\n",
      "          1       0.85      0.97      0.91      1930\n",
      "          2       0.85      0.49      0.62       407\n",
      "\n",
      "avg / total       0.83      0.85      0.82      2478\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 69.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.16      0.25       154\n",
      "          1       0.84      0.97      0.90      1915\n",
      "          2       0.80      0.45      0.57       409\n",
      "\n",
      "avg / total       0.82      0.83      0.81      2478\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 66.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.13      0.21       135\n",
      "          1       0.84      0.97      0.90      1907\n",
      "          2       0.82      0.45      0.58       436\n",
      "\n",
      "avg / total       0.82      0.83      0.81      2478\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 72.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.13      0.21       133\n",
      "          1       0.86      0.97      0.91      1937\n",
      "          2       0.81      0.49      0.61       408\n",
      "\n",
      "avg / total       0.84      0.85      0.82      2478\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 68.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.42      0.13      0.20       124\n",
      "          1       0.86      0.98      0.91      1942\n",
      "          2       0.86      0.50      0.63       412\n",
      "\n",
      "avg / total       0.84      0.85      0.83      2478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SVM Classifier\n",
    "C = [ 2e-2, 2e-1, 2e-0]\n",
    "gama = [ 2e-2, 2e-1, 2e-0]\n",
    "param = [{'svc__kernel': ['rbf'], 'svc__C': C}]\n",
    "clf = svm.SVC(decision_function_shape='ovo')\n",
    "scale = StandardScaler()\n",
    "pipeline = Pipeline([('scaler', scale), ('svc', clf)])\n",
    "\n",
    "svm_err = nested_kfold_cv(pipeline, param, X, Y)\n",
    "#param_grid = [{}] \n",
    "#grid_search = GridSearchCV(pipeline, \n",
    "#                           param_grid,\n",
    "#                          n_jobs = 5,\n",
    "#                           cv=StratifiedKFold(n_splits=5, \n",
    "#                                              random_state=42).split(X_train, y_train), \n",
    "#                           verbose=2)\n",
    "#model = grid_search.fit(X_train,y_train)\n",
    "#y_pred = model.predict(X_test)\n",
    "#report = classification_report( y_test, y_pred )\n",
    "#print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.10      0.17       164\n",
      "          1       0.85      0.97      0.90      1905\n",
      "          2       0.80      0.54      0.65       410\n",
      "\n",
      "avg / total       0.82      0.84      0.81      2479\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.47      0.12      0.19       127\n",
      "          1       0.86      0.97      0.91      1927\n",
      "          2       0.80      0.50      0.62       425\n",
      "\n",
      "avg / total       0.83      0.85      0.82      2479\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.10      0.17       136\n",
      "          1       0.86      0.97      0.91      1916\n",
      "          2       0.81      0.55      0.66       427\n",
      "\n",
      "avg / total       0.83      0.85      0.83      2479\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.15      0.24       145\n",
      "          1       0.87      0.96      0.91      1917\n",
      "          2       0.80      0.58      0.67       416\n",
      "\n",
      "avg / total       0.84      0.85      0.83      2478\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.13      0.21       171\n",
      "          1       0.84      0.97      0.90      1894\n",
      "          2       0.80      0.50      0.62       413\n",
      "\n",
      "avg / total       0.82      0.83      0.81      2478\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.12      0.20       141\n",
      "          1       0.86      0.97      0.91      1930\n",
      "          2       0.82      0.56      0.66       407\n",
      "\n",
      "avg / total       0.84      0.86      0.83      2478\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.12      0.19       154\n",
      "          1       0.86      0.97      0.91      1915\n",
      "          2       0.79      0.55      0.65       409\n",
      "\n",
      "avg / total       0.82      0.84      0.82      2478\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.41      0.11      0.17       135\n",
      "          1       0.85      0.96      0.90      1907\n",
      "          2       0.80      0.55      0.65       436\n",
      "\n",
      "avg / total       0.82      0.84      0.82      2478\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.11      0.17       133\n",
      "          1       0.86      0.97      0.91      1937\n",
      "          2       0.79      0.55      0.65       408\n",
      "\n",
      "avg / total       0.83      0.85      0.83      2478\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.17      0.26       124\n",
      "          1       0.88      0.98      0.92      1942\n",
      "          2       0.86      0.58      0.70       412\n",
      "\n",
      "avg / total       0.86      0.87      0.85      2478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "C = [2e-3, 2e-2, 2e-1, 2e-0, 2e-1, 2e-2, 2e-3]\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "clf = LogisticRegression(multi_class='ovr',solver='newton-cg')\n",
    "pipeline = Pipeline([('scaler', std_scaler)  , ('clf', clf)])\n",
    "param = [{'clf__C': C}]\n",
    "\n",
    "logreg_err = nested_kfold_cv(pipeline, param, X, Y)\n",
    "\n",
    "\n",
    "\n",
    "#TODO: logicstic regression, model selection, kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logreg_err' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-768ce2fd6f0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mttest_rel\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpaired_t_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mt_stat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpaired_t_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogreg_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvm_err\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mp_val\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logreg_err' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_rel as paired_t_test\n",
    "\n",
    "t_stat, p_val = paired_t_test(logreg_err, svm_err)\n",
    "\n",
    "if p_val <= 0.05:\n",
    "    print('\\np-value = ', p_val, ', hipoteza se odbacuje.')\n",
    "else:\n",
    "    print('\\np-value = ', p_val, ', hipoteza se ne odbacuje.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14218</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Pop a nigguh, sound like crisco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13906</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ok. Did this bitch just say wut I think she.sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>&amp;#9733; BEST ASIAN MASSAGE ON THE Brooklyn Par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Early bird catches the worm &amp;#128076;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21673</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Subtweet me one more time, you dirty chink whore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19381</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @ella__fraser: When you clam up, my surviva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3170</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>@EJTHR33 You do got alla da hoes..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>23126</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>What's worse than #Ebola ? The F-ing retards t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6469</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>@joshbuddy blue cheese crackers intrigues and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15797</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @HamseAbdala: Charlie Sheen is a fucking bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>17687</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @SheswantstheD: If you take your girl back ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15355</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @DinO_off_AD: Them hoes come them hoes go i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5465</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@_IAMKEN fuck Brett Farve redneck ass, he stuc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20833</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @xoxo_shelby_22: &amp;#8220;@zzachbarness: I'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11054</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>I need a bad bitch thats gon treat me good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9393</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Floppy bird chalmers over here taking after hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18044</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @TheCockiestMan: \"She's really nice when yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>272</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>\"@UberFacts: 15 sad TV character deaths we're ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>15258</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @DabGodBaby: But only BBC, the American ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>16994</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @NavyAssLos: Hunter Hurt Helmsley RT @Rich_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>17737</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @Smart_Cookie86: If I had to pick a Preside...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2891</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@CharleeRedz13 your gay stfu, dumb monkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23943</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>You're a bad bitch and I want you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14631</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @BadNewsAli: Look at ya boy @Quanb24 RT @Dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2592</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>@BabyxShad hoes twerking heavy to that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>11570</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>I'm going to dress up as trash for Halloween. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>19999</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @lmfaohilarious: Never go full retard http:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5415</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@ZaneIsClasker the fuck r u talking about I to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2064</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>' Cause these hoes ain't loyal ! And niggahs a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7806</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>All my bitches love me, all my all my bitches ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4260</th>\n",
       "      <td>14625</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @BaabyD_: lol im off this brooo. bitches ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4261</th>\n",
       "      <td>6222</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>@grinand http://www.youtube.com/watch?v=LRtKAQ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4262</th>\n",
       "      <td>20788</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @worley_7: Enough with the mind fuck and pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4263</th>\n",
       "      <td>19090</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @boburnham: In the future, gay culture will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4264</th>\n",
       "      <td>4802</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@ScoutingForFifa @TheBurntChip @JoelBurtFifa @...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4265</th>\n",
       "      <td>17262</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @PattyChocMilk: #teabagger math http://t.co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4266</th>\n",
       "      <td>16811</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @MichaelArenella: Shy girls are cute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4267</th>\n",
       "      <td>16734</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @Makeit_aMiller: @Mike_Stud @HoodieAllen ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4268</th>\n",
       "      <td>20058</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @magicjuice: #giantduck #pittsburghduck #ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4269</th>\n",
       "      <td>384</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>\"@rhythmixx_: I told Mariam to but cigarettes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4270</th>\n",
       "      <td>17780</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @SpacePlankton: This monkey on my back keep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4271</th>\n",
       "      <td>4901</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@Sterm26 @RealJamesWoods Just like when I call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4272</th>\n",
       "      <td>8525</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Calling out teabaggers, conservatives on their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4273</th>\n",
       "      <td>8462</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Burnating all Yankees and their thatch roof co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4274</th>\n",
       "      <td>14091</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Pantera - 5 Minutes Alone http://t.co/SrN5Yc16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4275</th>\n",
       "      <td>750</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>#Minnesota is full of white trash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4276</th>\n",
       "      <td>13018</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Magellan? You suck at trash talk. @physguy2 @s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4277</th>\n",
       "      <td>19813</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @jrsalzman: Buzzfeed males who live in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4278</th>\n",
       "      <td>18214</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @TopBlokeBill: Are you Justin Beiber? Becau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4279</th>\n",
       "      <td>8847</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Damn Charlie Wilson is still around?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4280</th>\n",
       "      <td>19292</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @delphrano: Berto couldn't KO this dude who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4281</th>\n",
       "      <td>13159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Mi caldito de pollo estuvo incredible!! Ahora ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4282</th>\n",
       "      <td>16187</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @Jimmy_Santander: \"Deme gallo pinto con car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4283</th>\n",
       "      <td>1325</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>&amp;#8220;@HeyKeifer: Golden Oreos &amp;#128016; regu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4284</th>\n",
       "      <td>4588</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>@QueChele u know how Hispanics be takin All th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4285</th>\n",
       "      <td>7386</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>@tmundal just because some moist bint lobbed a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4286</th>\n",
       "      <td>6630</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>@kieffer_jason bitch who's playing am not worr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4287</th>\n",
       "      <td>5148</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>@TibaBurger: Straight off the bench #ned #Worl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4288</th>\n",
       "      <td>1467</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&amp;#8220;@NiHao_Rocky Who is she? RT @DrTruth247...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4289</th>\n",
       "      <td>23203</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>When people label their name on there ghetto n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4290 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0          14218      3            2                   1        0      0   \n",
       "1          13906      3            2                   1        0      0   \n",
       "2           2006      3            0                   0        3      2   \n",
       "3           9159      3            0                   0        3      2   \n",
       "4          21673      3            3                   0        0      0   \n",
       "5          19381      3            0                   0        3      2   \n",
       "6           3170      3            0                   3        0      1   \n",
       "7          23126      3            2                   1        0      0   \n",
       "8           6469      3            0                   0        3      2   \n",
       "9          15797      3            0                   3        0      1   \n",
       "10         17687      3            0                   3        0      1   \n",
       "11         15355      3            0                   3        0      1   \n",
       "12          5465      3            2                   1        0      0   \n",
       "13         20833      3            0                   3        0      1   \n",
       "14         11054      3            0                   2        1      1   \n",
       "15          9393      3            0                   0        3      2   \n",
       "16         18044      3            0                   3        0      1   \n",
       "17           272      3            0                   0        3      2   \n",
       "18         15258      6            0                   1        5      2   \n",
       "19         16994      3            0                   1        2      2   \n",
       "20         17737      3            0                   0        3      2   \n",
       "21          2891      3            2                   1        0      0   \n",
       "22         23943      3            1                   2        0      1   \n",
       "23         14631      3            2                   1        0      0   \n",
       "24          2592      3            0                   3        0      1   \n",
       "25         11570      3            0                   1        2      2   \n",
       "26         19999      3            2                   1        0      0   \n",
       "27          5415      3            2                   1        0      0   \n",
       "28          2064      3            1                   2        0      1   \n",
       "29          7806      3            0                   3        0      1   \n",
       "...          ...    ...          ...                 ...      ...    ...   \n",
       "4260       14625      3            0                   3        0      1   \n",
       "4261        6222      3            0                   0        3      2   \n",
       "4262       20788      6            1                   5        0      1   \n",
       "4263       19090      3            2                   1        0      0   \n",
       "4264        4802      3            2                   1        0      0   \n",
       "4265       17262      3            0                   1        2      2   \n",
       "4266       16811      3            0                   0        3      2   \n",
       "4267       16734      4            0                   4        0      1   \n",
       "4268       20058      3            0                   0        3      2   \n",
       "4269         384      3            1                   0        2      2   \n",
       "4270       17780      3            0                   1        2      2   \n",
       "4271        4901      3            2                   1        0      0   \n",
       "4272        8525      3            0                   0        3      2   \n",
       "4273        8462      3            1                   0        2      2   \n",
       "4274       14091      3            2                   1        0      0   \n",
       "4275         750      3            3                   0        0      0   \n",
       "4276       13018      3            0                   0        3      2   \n",
       "4277       19813      3            0                   0        3      2   \n",
       "4278       18214      3            3                   0        0      0   \n",
       "4279        8847      6            0                   1        5      2   \n",
       "4280       19292      3            0                   0        3      2   \n",
       "4281       13159      3            0                   1        2      2   \n",
       "4282       16187      3            0                   0        3      2   \n",
       "4283        1325      3            0                   0        3      2   \n",
       "4284        4588      3            1                   2        0      1   \n",
       "4285        7386      3            0                   0        3      2   \n",
       "4286        6630      3            0                   3        0      1   \n",
       "4287        5148      3            0                   0        3      2   \n",
       "4288        1467      3            2                   1        0      0   \n",
       "4289       23203      3            0                   1        2      2   \n",
       "\n",
       "                                                  tweet  \n",
       "0                       Pop a nigguh, sound like crisco  \n",
       "1     Ok. Did this bitch just say wut I think she.sa...  \n",
       "2     &#9733; BEST ASIAN MASSAGE ON THE Brooklyn Par...  \n",
       "3                 Early bird catches the worm &#128076;  \n",
       "4      Subtweet me one more time, you dirty chink whore  \n",
       "5     RT @ella__fraser: When you clam up, my surviva...  \n",
       "6                    @EJTHR33 You do got alla da hoes..  \n",
       "7     What's worse than #Ebola ? The F-ing retards t...  \n",
       "8     @joshbuddy blue cheese crackers intrigues and ...  \n",
       "9     RT @HamseAbdala: Charlie Sheen is a fucking bo...  \n",
       "10    RT @SheswantstheD: If you take your girl back ...  \n",
       "11    RT @DinO_off_AD: Them hoes come them hoes go i...  \n",
       "12    @_IAMKEN fuck Brett Farve redneck ass, he stuc...  \n",
       "13    RT @xoxo_shelby_22: &#8220;@zzachbarness: I'll...  \n",
       "14           I need a bad bitch thats gon treat me good  \n",
       "15    Floppy bird chalmers over here taking after hi...  \n",
       "16    RT @TheCockiestMan: \"She's really nice when yo...  \n",
       "17    \"@UberFacts: 15 sad TV character deaths we're ...  \n",
       "18    RT @DabGodBaby: But only BBC, the American ver...  \n",
       "19    RT @NavyAssLos: Hunter Hurt Helmsley RT @Rich_...  \n",
       "20    RT @Smart_Cookie86: If I had to pick a Preside...  \n",
       "21            @CharleeRedz13 your gay stfu, dumb monkey  \n",
       "22                   You're a bad bitch and I want you.  \n",
       "23    RT @BadNewsAli: Look at ya boy @Quanb24 RT @Dr...  \n",
       "24               @BabyxShad hoes twerking heavy to that  \n",
       "25    I'm going to dress up as trash for Halloween. ...  \n",
       "26    RT @lmfaohilarious: Never go full retard http:...  \n",
       "27    @ZaneIsClasker the fuck r u talking about I to...  \n",
       "28    ' Cause these hoes ain't loyal ! And niggahs a...  \n",
       "29    All my bitches love me, all my all my bitches ...  \n",
       "...                                                 ...  \n",
       "4260  RT @BaabyD_: lol im off this brooo. bitches ja...  \n",
       "4261  @grinand http://www.youtube.com/watch?v=LRtKAQ...  \n",
       "4262  RT @worley_7: Enough with the mind fuck and pa...  \n",
       "4263  RT @boburnham: In the future, gay culture will...  \n",
       "4264  @ScoutingForFifa @TheBurntChip @JoelBurtFifa @...  \n",
       "4265  RT @PattyChocMilk: #teabagger math http://t.co...  \n",
       "4266            RT @MichaelArenella: Shy girls are cute  \n",
       "4267  RT @Makeit_aMiller: @Mike_Stud @HoodieAllen ho...  \n",
       "4268  RT @magicjuice: #giantduck #pittsburghduck #ye...  \n",
       "4269  \"@rhythmixx_: I told Mariam to but cigarettes ...  \n",
       "4270  RT @SpacePlankton: This monkey on my back keep...  \n",
       "4271  @Sterm26 @RealJamesWoods Just like when I call...  \n",
       "4272  Calling out teabaggers, conservatives on their...  \n",
       "4273  Burnating all Yankees and their thatch roof co...  \n",
       "4274  Pantera - 5 Minutes Alone http://t.co/SrN5Yc16...  \n",
       "4275                  #Minnesota is full of white trash  \n",
       "4276  Magellan? You suck at trash talk. @physguy2 @s...  \n",
       "4277  RT @jrsalzman: Buzzfeed males who live in the ...  \n",
       "4278  RT @TopBlokeBill: Are you Justin Beiber? Becau...  \n",
       "4279               Damn Charlie Wilson is still around?  \n",
       "4280  RT @delphrano: Berto couldn't KO this dude who...  \n",
       "4281  Mi caldito de pollo estuvo incredible!! Ahora ...  \n",
       "4282  RT @Jimmy_Santander: \"Deme gallo pinto con car...  \n",
       "4283  &#8220;@HeyKeifer: Golden Oreos &#128016; regu...  \n",
       "4284  @QueChele u know how Hispanics be takin All th...  \n",
       "4285  @tmundal just because some moist bint lobbed a...  \n",
       "4286  @kieffer_jason bitch who's playing am not worr...  \n",
       "4287  @TibaBurger: Straight off the bench #ned #Worl...  \n",
       "4288  &#8220;@NiHao_Rocky Who is she? RT @DrTruth247...  \n",
       "4289  When people label their name on there ghetto n...  \n",
       "\n",
       "[4290 rows x 7 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sampling data so we get equal distribution for all classes. used in active learning\n",
    "d0 = dataset[dataset['class'] == 0]\n",
    "d1 = dataset[dataset['class'] == 1].sample(d0.shape[0])\n",
    "d2 = dataset[dataset['class'] == 2].sample(d0.shape[0])\n",
    "\n",
    "#Dl data that is representing same number of all classes. From this we will take n data for starter training as\n",
    "#labeled data, and rest for unlabeled data for active learning\n",
    "Dl = pd.concat([d0,d1,d2])\n",
    "Dl = sklearn.utils.shuffle(Dl)\n",
    "Dl.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New kfold\n",
      "Label Spreading model: 1116 labeled & 21188 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.11      0.04      0.06       164\n",
      "          1       0.80      0.91      0.85      1905\n",
      "          2       0.47      0.30      0.37       410\n",
      "\n",
      "avg / total       0.70      0.75      0.72      2479\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\semi_supervised\\label_propagation.py:201: RuntimeWarning: invalid value encountered in true_divide\n",
      "  probabilities /= normalizer\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:26: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_maximum(a, axis, None, out, keepdims)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:60: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Spreading model: 2048 labeled & 20256 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.19      0.09      0.12       164\n",
      "          1       0.82      0.90      0.86      1905\n",
      "          2       0.53      0.40      0.45       410\n",
      "\n",
      "avg / total       0.73      0.76      0.74      2479\n",
      "\n",
      "Label Spreading model: 2971 labeled & 19333 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.27      0.12      0.17       164\n",
      "          1       0.82      0.90      0.86      1905\n",
      "          2       0.52      0.40      0.45       410\n",
      "\n",
      "avg / total       0.74      0.77      0.75      2479\n",
      "\n",
      "Label Spreading model: 3843 labeled & 18461 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.23      0.13      0.17       164\n",
      "          1       0.82      0.90      0.86      1905\n",
      "          2       0.52      0.37      0.43       410\n",
      "\n",
      "avg / total       0.73      0.76      0.74      2479\n",
      "\n",
      "Label Spreading model: 4711 labeled & 17593 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.30      0.16      0.21       164\n",
      "          1       0.82      0.90      0.86      1905\n",
      "          2       0.49      0.36      0.41       410\n",
      "\n",
      "avg / total       0.73      0.76      0.74      2479\n",
      "\n",
      "Label Spreading model: 5477 labeled & 16827 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.31      0.16      0.22       164\n",
      "          1       0.82      0.91      0.86      1905\n",
      "          2       0.54      0.35      0.42       410\n",
      "\n",
      "avg / total       0.74      0.77      0.75      2479\n",
      "\n",
      "Label Spreading model: 6244 labeled & 16060 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.35      0.20      0.25       164\n",
      "          1       0.82      0.91      0.86      1905\n",
      "          2       0.53      0.36      0.43       410\n",
      "\n",
      "avg / total       0.74      0.77      0.75      2479\n",
      "\n",
      "Label Spreading model: 7002 labeled & 15302 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.36      0.18      0.24       164\n",
      "          1       0.82      0.91      0.86      1905\n",
      "          2       0.53      0.36      0.43       410\n",
      "\n",
      "avg / total       0.74      0.77      0.75      2479\n",
      "\n",
      "Label Spreading model: 7736 labeled & 14568 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.34      0.18      0.24       164\n",
      "          1       0.82      0.90      0.86      1905\n",
      "          2       0.50      0.37      0.42       410\n",
      "\n",
      "avg / total       0.74      0.77      0.75      2479\n",
      "\n",
      "Label Spreading model: 8476 labeled & 13828 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.37      0.19      0.25       164\n",
      "          1       0.82      0.91      0.86      1905\n",
      "          2       0.51      0.36      0.42       410\n",
      "\n",
      "avg / total       0.74      0.77      0.75      2479\n",
      "\n",
      "New kfold\n",
      "Label Spreading model: 1116 labeled & 21188 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.28      0.09      0.14       127\n",
      "          1       0.81      0.91      0.86      1927\n",
      "          2       0.45      0.31      0.37       425\n",
      "\n",
      "avg / total       0.72      0.76      0.74      2479\n",
      "\n",
      "Label Spreading model: 2062 labeled & 20242 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.25      0.19      0.21       127\n",
      "          1       0.83      0.89      0.86      1927\n",
      "          2       0.50      0.38      0.43       425\n",
      "\n",
      "avg / total       0.74      0.77      0.75      2479\n",
      "\n",
      "Label Spreading model: 2928 labeled & 19376 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.21      0.12      0.15       127\n",
      "          1       0.83      0.91      0.87      1927\n",
      "          2       0.53      0.36      0.43       425\n",
      "\n",
      "avg / total       0.74      0.77      0.75      2479\n",
      "\n",
      "Label Spreading model: 3756 labeled & 18548 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.27      0.18      0.22       127\n",
      "          1       0.83      0.89      0.86      1927\n",
      "          2       0.51      0.39      0.44       425\n",
      "\n",
      "avg / total       0.75      0.77      0.76      2479\n",
      "\n",
      "Label Spreading model: 4576 labeled & 17728 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.30      0.19      0.23       127\n",
      "          1       0.83      0.89      0.86      1927\n",
      "          2       0.50      0.38      0.43       425\n",
      "\n",
      "avg / total       0.75      0.77      0.75      2479\n",
      "\n",
      "Label Spreading model: 5299 labeled & 17005 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.27      0.17      0.20       127\n",
      "          1       0.83      0.90      0.86      1927\n",
      "          2       0.49      0.36      0.42       425\n",
      "\n",
      "avg / total       0.74      0.77      0.75      2479\n",
      "\n",
      "Label Spreading model: 5989 labeled & 16315 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.27      0.17      0.21       127\n",
      "          1       0.83      0.91      0.87      1927\n",
      "          2       0.52      0.35      0.42       425\n",
      "\n",
      "avg / total       0.75      0.77      0.76      2479\n",
      "\n",
      "Label Spreading model: 6749 labeled & 15555 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.24      0.15      0.19       127\n",
      "          1       0.83      0.91      0.86      1927\n",
      "          2       0.51      0.34      0.41       425\n",
      "\n",
      "avg / total       0.74      0.77      0.75      2479\n",
      "\n",
      "Label Spreading model: 7501 labeled & 14803 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.24      0.15      0.18       127\n",
      "          1       0.83      0.91      0.87      1927\n",
      "          2       0.52      0.35      0.42       425\n",
      "\n",
      "avg / total       0.74      0.77      0.75      2479\n",
      "\n",
      "Label Spreading model: 8221 labeled & 14083 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.22      0.14      0.17       127\n",
      "          1       0.83      0.90      0.87      1927\n",
      "          2       0.52      0.36      0.43       425\n",
      "\n",
      "avg / total       0.75      0.77      0.75      2479\n",
      "\n",
      "New kfold\n",
      "Label Spreading model: 1116 labeled & 21188 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.16      0.05      0.08       136\n",
      "          1       0.82      0.91      0.86      1916\n",
      "          2       0.50      0.34      0.40       427\n",
      "\n",
      "avg / total       0.72      0.77      0.74      2479\n",
      "\n",
      "Label Spreading model: 2057 labeled & 20247 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.23      0.10      0.14       136\n",
      "          1       0.82      0.91      0.86      1916\n",
      "          2       0.53      0.36      0.43       427\n",
      "\n",
      "avg / total       0.74      0.77      0.75      2479\n",
      "\n",
      "Label Spreading model: 2968 labeled & 19336 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.31      0.22      0.26       136\n",
      "          1       0.83      0.89      0.86      1916\n",
      "          2       0.52      0.39      0.44       427\n",
      "\n",
      "avg / total       0.75      0.77      0.76      2479\n",
      "\n",
      "Label Spreading model: 3833 labeled & 18471 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.32      0.21      0.26       136\n",
      "          1       0.83      0.89      0.86      1916\n",
      "          2       0.51      0.38      0.43       427\n",
      "\n",
      "avg / total       0.74      0.77      0.75      2479\n",
      "\n",
      "Label Spreading model: 4610 labeled & 17694 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.29      0.23      0.26       136\n",
      "          1       0.83      0.90      0.86      1916\n",
      "          2       0.56      0.39      0.46       427\n",
      "\n",
      "avg / total       0.75      0.77      0.76      2479\n",
      "\n",
      "Label Spreading model: 5353 labeled & 16951 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.30      0.19      0.23       136\n",
      "          1       0.83      0.90      0.86      1916\n",
      "          2       0.53      0.39      0.45       427\n",
      "\n",
      "avg / total       0.75      0.77      0.76      2479\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Spreading model: 6030 labeled & 16274 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.29      0.21      0.24       136\n",
      "          1       0.83      0.90      0.86      1916\n",
      "          2       0.54      0.39      0.45       427\n",
      "\n",
      "avg / total       0.75      0.77      0.76      2479\n",
      "\n",
      "Label Spreading model: 6675 labeled & 15629 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.28      0.20      0.23       136\n",
      "          1       0.83      0.90      0.86      1916\n",
      "          2       0.55      0.40      0.46       427\n",
      "\n",
      "avg / total       0.75      0.77      0.76      2479\n",
      "\n",
      "Label Spreading model: 7282 labeled & 15022 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.28      0.21      0.24       136\n",
      "          1       0.83      0.90      0.86      1916\n",
      "          2       0.56      0.40      0.46       427\n",
      "\n",
      "avg / total       0.75      0.78      0.76      2479\n",
      "\n",
      "Label Spreading model: 7870 labeled & 14434 unlabeled (22304 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.29      0.22      0.25       136\n",
      "          1       0.83      0.90      0.87      1916\n",
      "          2       0.57      0.39      0.46       427\n",
      "\n",
      "avg / total       0.76      0.78      0.76      2479\n",
      "\n",
      "New kfold\n",
      "Label Spreading model: 1116 labeled & 21189 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.07      0.03      0.04       145\n",
      "          1       0.82      0.89      0.85      1917\n",
      "          2       0.49      0.39      0.43       416\n",
      "\n",
      "avg / total       0.72      0.76      0.73      2478\n",
      "\n",
      "Label Spreading model: 2054 labeled & 20251 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.17      0.10      0.13       145\n",
      "          1       0.82      0.90      0.86      1917\n",
      "          2       0.56      0.38      0.46       416\n",
      "\n",
      "avg / total       0.74      0.77      0.75      2478\n",
      "\n",
      "Label Spreading model: 2905 labeled & 19400 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.17      0.10      0.13       145\n",
      "          1       0.82      0.90      0.86      1917\n",
      "          2       0.54      0.38      0.45       416\n",
      "\n",
      "avg / total       0.74      0.77      0.75      2478\n",
      "\n",
      "Label Spreading model: 3761 labeled & 18544 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.25      0.17      0.20       145\n",
      "          1       0.83      0.90      0.86      1917\n",
      "          2       0.54      0.40      0.46       416\n",
      "\n",
      "avg / total       0.75      0.77      0.75      2478\n",
      "\n",
      "Label Spreading model: 4598 labeled & 17707 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.26      0.19      0.22       145\n",
      "          1       0.83      0.89      0.86      1917\n",
      "          2       0.51      0.41      0.46       416\n",
      "\n",
      "avg / total       0.74      0.77      0.75      2478\n",
      "\n",
      "Label Spreading model: 5419 labeled & 16886 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.29      0.19      0.23       145\n",
      "          1       0.83      0.89      0.86      1917\n",
      "          2       0.52      0.41      0.46       416\n",
      "\n",
      "avg / total       0.75      0.77      0.76      2478\n",
      "\n",
      "Label Spreading model: 6195 labeled & 16110 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.27      0.17      0.21       145\n",
      "          1       0.83      0.90      0.86      1917\n",
      "          2       0.53      0.40      0.46       416\n",
      "\n",
      "avg / total       0.75      0.77      0.76      2478\n",
      "\n",
      "Label Spreading model: 6963 labeled & 15342 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.30      0.21      0.24       145\n",
      "          1       0.84      0.90      0.87      1917\n",
      "          2       0.56      0.42      0.48       416\n",
      "\n",
      "avg / total       0.76      0.78      0.76      2478\n",
      "\n",
      "Label Spreading model: 7643 labeled & 14662 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.27      0.20      0.23       145\n",
      "          1       0.84      0.89      0.86      1917\n",
      "          2       0.55      0.42      0.47       416\n",
      "\n",
      "avg / total       0.75      0.77      0.76      2478\n",
      "\n",
      "Label Spreading model: 8320 labeled & 13985 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.26      0.19      0.22       145\n",
      "          1       0.84      0.90      0.86      1917\n",
      "          2       0.55      0.43      0.48       416\n",
      "\n",
      "avg / total       0.75      0.77      0.76      2478\n",
      "\n",
      "New kfold\n",
      "Label Spreading model: 1116 labeled & 21189 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.27      0.08      0.13       171\n",
      "          1       0.80      0.94      0.86      1894\n",
      "          2       0.54      0.28      0.37       413\n",
      "\n",
      "avg / total       0.72      0.77      0.73      2478\n",
      "\n",
      "Label Spreading model: 2066 labeled & 20239 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.43      0.23      0.30       171\n",
      "          1       0.82      0.92      0.87      1894\n",
      "          2       0.55      0.36      0.43       413\n",
      "\n",
      "avg / total       0.75      0.78      0.76      2478\n",
      "\n",
      "Label Spreading model: 2957 labeled & 19348 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.42      0.15      0.22       171\n",
      "          1       0.82      0.94      0.87      1894\n",
      "          2       0.57      0.36      0.44       413\n",
      "\n",
      "avg / total       0.75      0.79      0.76      2478\n",
      "\n",
      "Label Spreading model: 3788 labeled & 18517 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.34      0.16      0.22       171\n",
      "          1       0.82      0.92      0.87      1894\n",
      "          2       0.56      0.36      0.44       413\n",
      "\n",
      "avg / total       0.74      0.78      0.75      2478\n",
      "\n",
      "Label Spreading model: 4498 labeled & 17807 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.33      0.15      0.21       171\n",
      "          1       0.83      0.92      0.87      1894\n",
      "          2       0.55      0.39      0.46       413\n",
      "\n",
      "avg / total       0.75      0.78      0.76      2478\n",
      "\n",
      "Label Spreading model: 5180 labeled & 17125 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.35      0.15      0.21       171\n",
      "          1       0.82      0.92      0.87      1894\n",
      "          2       0.54      0.38      0.45       413\n",
      "\n",
      "avg / total       0.74      0.78      0.75      2478\n",
      "\n",
      "Label Spreading model: 5911 labeled & 16394 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.30      0.13      0.19       171\n",
      "          1       0.82      0.92      0.87      1894\n",
      "          2       0.55      0.38      0.45       413\n",
      "\n",
      "avg / total       0.74      0.77      0.75      2478\n",
      "\n",
      "Label Spreading model: 6636 labeled & 15669 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.38      0.18      0.24       171\n",
      "          1       0.82      0.92      0.87      1894\n",
      "          2       0.56      0.36      0.44       413\n",
      "\n",
      "avg / total       0.75      0.78      0.75      2478\n",
      "\n",
      "Label Spreading model: 7346 labeled & 14959 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.35      0.16      0.22       171\n",
      "          1       0.82      0.92      0.87      1894\n",
      "          2       0.55      0.37      0.44       413\n",
      "\n",
      "avg / total       0.74      0.77      0.75      2478\n",
      "\n",
      "Label Spreading model: 8001 labeled & 14304 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.36      0.15      0.21       171\n",
      "          1       0.82      0.92      0.87      1894\n",
      "          2       0.54      0.36      0.43       413\n",
      "\n",
      "avg / total       0.74      0.77      0.75      2478\n",
      "\n",
      "New kfold\n",
      "Label Spreading model: 1116 labeled & 21189 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.24      0.08      0.12       141\n",
      "          1       0.82      0.93      0.87      1930\n",
      "          2       0.53      0.31      0.40       407\n",
      "\n",
      "avg / total       0.74      0.78      0.75      2478\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Spreading model: 2060 labeled & 20245 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.24      0.21      0.22       141\n",
      "          1       0.83      0.90      0.86      1930\n",
      "          2       0.55      0.35      0.43       407\n",
      "\n",
      "avg / total       0.75      0.77      0.76      2478\n",
      "\n",
      "Label Spreading model: 2944 labeled & 19361 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.31      0.22      0.26       141\n",
      "          1       0.84      0.91      0.87      1930\n",
      "          2       0.57      0.40      0.47       407\n",
      "\n",
      "avg / total       0.76      0.79      0.77      2478\n",
      "\n",
      "Label Spreading model: 3785 labeled & 18520 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.28      0.19      0.23       141\n",
      "          1       0.84      0.90      0.87      1930\n",
      "          2       0.56      0.42      0.48       407\n",
      "\n",
      "avg / total       0.76      0.78      0.77      2478\n",
      "\n",
      "Label Spreading model: 4624 labeled & 17681 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.30      0.19      0.23       141\n",
      "          1       0.84      0.90      0.87      1930\n",
      "          2       0.53      0.41      0.46       407\n",
      "\n",
      "avg / total       0.76      0.78      0.77      2478\n",
      "\n",
      "Label Spreading model: 5431 labeled & 16874 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.32      0.19      0.24       141\n",
      "          1       0.84      0.91      0.87      1930\n",
      "          2       0.54      0.40      0.46       407\n",
      "\n",
      "avg / total       0.76      0.78      0.77      2478\n",
      "\n",
      "Label Spreading model: 6213 labeled & 16092 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.34      0.20      0.25       141\n",
      "          1       0.84      0.91      0.87      1930\n",
      "          2       0.54      0.40      0.46       407\n",
      "\n",
      "avg / total       0.76      0.79      0.77      2478\n",
      "\n",
      "Label Spreading model: 6968 labeled & 15337 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.35      0.19      0.25       141\n",
      "          1       0.84      0.91      0.87      1930\n",
      "          2       0.53      0.40      0.45       407\n",
      "\n",
      "avg / total       0.76      0.78      0.77      2478\n",
      "\n",
      "Label Spreading model: 7729 labeled & 14576 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.33      0.19      0.24       141\n",
      "          1       0.84      0.91      0.87      1930\n",
      "          2       0.53      0.40      0.45       407\n",
      "\n",
      "avg / total       0.76      0.78      0.77      2478\n",
      "\n",
      "Label Spreading model: 8438 labeled & 13867 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.34      0.21      0.26       141\n",
      "          1       0.84      0.90      0.87      1930\n",
      "          2       0.52      0.42      0.46       407\n",
      "\n",
      "avg / total       0.76      0.78      0.77      2478\n",
      "\n",
      "New kfold\n",
      "Label Spreading model: 1116 labeled & 21189 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.15      0.05      0.07       154\n",
      "          1       0.82      0.91      0.86      1915\n",
      "          2       0.49      0.36      0.42       409\n",
      "\n",
      "avg / total       0.72      0.76      0.74      2478\n",
      "\n",
      "Label Spreading model: 2079 labeled & 20226 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.06      0.03      0.04       154\n",
      "          1       0.82      0.91      0.86      1915\n",
      "          2       0.53      0.38      0.44       409\n",
      "\n",
      "avg / total       0.72      0.77      0.74      2478\n",
      "\n",
      "Label Spreading model: 2975 labeled & 19330 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.18      0.12      0.15       154\n",
      "          1       0.83      0.89      0.86      1915\n",
      "          2       0.53      0.40      0.46       409\n",
      "\n",
      "avg / total       0.74      0.76      0.75      2478\n",
      "\n",
      "Label Spreading model: 3851 labeled & 18454 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.18      0.11      0.14       154\n",
      "          1       0.83      0.89      0.86      1915\n",
      "          2       0.50      0.40      0.45       409\n",
      "\n",
      "avg / total       0.74      0.76      0.75      2478\n",
      "\n",
      "Label Spreading model: 4702 labeled & 17603 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.27      0.18      0.21       154\n",
      "          1       0.83      0.90      0.86      1915\n",
      "          2       0.51      0.39      0.44       409\n",
      "\n",
      "avg / total       0.74      0.77      0.75      2478\n",
      "\n",
      "Label Spreading model: 5485 labeled & 16820 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.23      0.12      0.16       154\n",
      "          1       0.83      0.90      0.86      1915\n",
      "          2       0.51      0.41      0.45       409\n",
      "\n",
      "avg / total       0.74      0.77      0.75      2478\n",
      "\n",
      "Label Spreading model: 6254 labeled & 16051 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.26      0.12      0.16       154\n",
      "          1       0.83      0.91      0.87      1915\n",
      "          2       0.54      0.40      0.46       409\n",
      "\n",
      "avg / total       0.75      0.78      0.76      2478\n",
      "\n",
      "Label Spreading model: 7014 labeled & 15291 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.28      0.13      0.18       154\n",
      "          1       0.83      0.91      0.87      1915\n",
      "          2       0.53      0.38      0.44       409\n",
      "\n",
      "avg / total       0.74      0.78      0.76      2478\n",
      "\n",
      "Label Spreading model: 7686 labeled & 14619 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.30      0.14      0.19       154\n",
      "          1       0.83      0.92      0.87      1915\n",
      "          2       0.55      0.38      0.45       409\n",
      "\n",
      "avg / total       0.75      0.78      0.76      2478\n",
      "\n",
      "Label Spreading model: 8308 labeled & 13997 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.32      0.18      0.23       154\n",
      "          1       0.83      0.92      0.87      1915\n",
      "          2       0.57      0.39      0.46       409\n",
      "\n",
      "avg / total       0.76      0.78      0.76      2478\n",
      "\n",
      "New kfold\n",
      "Label Spreading model: 1116 labeled & 21189 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.28      0.11      0.16       135\n",
      "          1       0.82      0.91      0.86      1907\n",
      "          2       0.50      0.37      0.43       436\n",
      "\n",
      "avg / total       0.74      0.77      0.75      2478\n",
      "\n",
      "Label Spreading model: 2056 labeled & 20249 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.15      0.08      0.10       135\n",
      "          1       0.82      0.89      0.86      1907\n",
      "          2       0.51      0.40      0.45       436\n",
      "\n",
      "avg / total       0.73      0.76      0.74      2478\n",
      "\n",
      "Label Spreading model: 2939 labeled & 19366 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.20      0.15      0.17       135\n",
      "          1       0.83      0.89      0.86      1907\n",
      "          2       0.52      0.38      0.44       436\n",
      "\n",
      "avg / total       0.74      0.76      0.75      2478\n",
      "\n",
      "Label Spreading model: 3794 labeled & 18511 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.27      0.19      0.22       135\n",
      "          1       0.83      0.89      0.86      1907\n",
      "          2       0.52      0.40      0.45       436\n",
      "\n",
      "avg / total       0.74      0.77      0.75      2478\n",
      "\n",
      "Label Spreading model: 4602 labeled & 17703 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.25      0.16      0.19       135\n",
      "          1       0.83      0.89      0.86      1907\n",
      "          2       0.52      0.43      0.47       436\n",
      "\n",
      "avg / total       0.75      0.77      0.75      2478\n",
      "\n",
      "Label Spreading model: 5381 labeled & 16924 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.25      0.13      0.17       135\n",
      "          1       0.83      0.89      0.86      1907\n",
      "          2       0.50      0.42      0.46       436\n",
      "\n",
      "avg / total       0.74      0.76      0.75      2478\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Spreading model: 6180 labeled & 16125 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.25      0.10      0.15       135\n",
      "          1       0.83      0.90      0.86      1907\n",
      "          2       0.53      0.42      0.47       436\n",
      "\n",
      "avg / total       0.74      0.77      0.76      2478\n",
      "\n",
      "Label Spreading model: 6937 labeled & 15368 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.26      0.13      0.18       135\n",
      "          1       0.83      0.90      0.86      1907\n",
      "          2       0.54      0.43      0.48       436\n",
      "\n",
      "avg / total       0.75      0.77      0.76      2478\n",
      "\n",
      "Label Spreading model: 7634 labeled & 14671 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.25      0.11      0.15       135\n",
      "          1       0.83      0.90      0.86      1907\n",
      "          2       0.54      0.43      0.48       436\n",
      "\n",
      "avg / total       0.75      0.78      0.76      2478\n",
      "\n",
      "Label Spreading model: 8315 labeled & 13990 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.25      0.10      0.15       135\n",
      "          1       0.83      0.90      0.86      1907\n",
      "          2       0.53      0.43      0.47       436\n",
      "\n",
      "avg / total       0.74      0.77      0.76      2478\n",
      "\n",
      "New kfold\n",
      "Label Spreading model: 1116 labeled & 21189 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.11      0.04      0.06       133\n",
      "          1       0.82      0.91      0.86      1937\n",
      "          2       0.46      0.30      0.36       408\n",
      "\n",
      "avg / total       0.72      0.77      0.74      2478\n",
      "\n",
      "Label Spreading model: 2044 labeled & 20261 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.24      0.10      0.14       133\n",
      "          1       0.83      0.91      0.87      1937\n",
      "          2       0.50      0.37      0.42       408\n",
      "\n",
      "avg / total       0.74      0.78      0.76      2478\n",
      "\n",
      "Label Spreading model: 2939 labeled & 19366 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.19      0.03      0.05       133\n",
      "          1       0.83      0.92      0.88      1937\n",
      "          2       0.51      0.40      0.45       408\n",
      "\n",
      "avg / total       0.75      0.79      0.76      2478\n",
      "\n",
      "Label Spreading model: 3823 labeled & 18482 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.33      0.11      0.16       133\n",
      "          1       0.83      0.91      0.87      1937\n",
      "          2       0.49      0.39      0.44       408\n",
      "\n",
      "avg / total       0.75      0.78      0.76      2478\n",
      "\n",
      "Label Spreading model: 4698 labeled & 17607 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.33      0.13      0.18       133\n",
      "          1       0.83      0.91      0.87      1937\n",
      "          2       0.50      0.37      0.42       408\n",
      "\n",
      "avg / total       0.75      0.78      0.76      2478\n",
      "\n",
      "Label Spreading model: 5487 labeled & 16818 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.36      0.12      0.18       133\n",
      "          1       0.83      0.92      0.87      1937\n",
      "          2       0.51      0.35      0.42       408\n",
      "\n",
      "avg / total       0.75      0.78      0.76      2478\n",
      "\n",
      "Label Spreading model: 6262 labeled & 16043 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.36      0.12      0.18       133\n",
      "          1       0.83      0.91      0.87      1937\n",
      "          2       0.48      0.38      0.42       408\n",
      "\n",
      "avg / total       0.75      0.78      0.76      2478\n",
      "\n",
      "Label Spreading model: 7010 labeled & 15295 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.19      0.06      0.09       133\n",
      "          1       0.83      0.91      0.87      1937\n",
      "          2       0.50      0.40      0.44       408\n",
      "\n",
      "avg / total       0.74      0.78      0.76      2478\n",
      "\n",
      "Label Spreading model: 7697 labeled & 14608 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.18      0.07      0.10       133\n",
      "          1       0.83      0.91      0.87      1937\n",
      "          2       0.51      0.39      0.44       408\n",
      "\n",
      "avg / total       0.74      0.78      0.76      2478\n",
      "\n",
      "Label Spreading model: 8337 labeled & 13968 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.22      0.09      0.13       133\n",
      "          1       0.83      0.91      0.87      1937\n",
      "          2       0.51      0.38      0.44       408\n",
      "\n",
      "avg / total       0.75      0.78      0.76      2478\n",
      "\n",
      "New kfold\n",
      "Label Spreading model: 1116 labeled & 21189 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.26      0.07      0.11       124\n",
      "          1       0.83      0.93      0.88      1942\n",
      "          2       0.54      0.34      0.42       412\n",
      "\n",
      "avg / total       0.75      0.79      0.76      2478\n",
      "\n",
      "Label Spreading model: 2061 labeled & 20244 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.22      0.10      0.13       124\n",
      "          1       0.84      0.90      0.87      1942\n",
      "          2       0.50      0.41      0.45       412\n",
      "\n",
      "avg / total       0.75      0.78      0.76      2478\n",
      "\n",
      "Label Spreading model: 2952 labeled & 19353 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.24      0.16      0.19       124\n",
      "          1       0.84      0.89      0.86      1942\n",
      "          2       0.47      0.37      0.41       412\n",
      "\n",
      "avg / total       0.75      0.77      0.75      2478\n",
      "\n",
      "Label Spreading model: 3813 labeled & 18492 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.23      0.12      0.16       124\n",
      "          1       0.84      0.90      0.87      1942\n",
      "          2       0.48      0.37      0.42       412\n",
      "\n",
      "avg / total       0.75      0.77      0.76      2478\n",
      "\n",
      "Label Spreading model: 4651 labeled & 17654 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.23      0.11      0.15       124\n",
      "          1       0.83      0.90      0.87      1942\n",
      "          2       0.47      0.35      0.40       412\n",
      "\n",
      "avg / total       0.74      0.77      0.75      2478\n",
      "\n",
      "Label Spreading model: 5441 labeled & 16864 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.25      0.15      0.18       124\n",
      "          1       0.83      0.90      0.86      1942\n",
      "          2       0.46      0.34      0.39       412\n",
      "\n",
      "avg / total       0.74      0.77      0.75      2478\n",
      "\n",
      "Label Spreading model: 6186 labeled & 16119 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.25      0.16      0.20       124\n",
      "          1       0.83      0.90      0.86      1942\n",
      "          2       0.49      0.36      0.41       412\n",
      "\n",
      "avg / total       0.75      0.77      0.76      2478\n",
      "\n",
      "Label Spreading model: 6880 labeled & 15425 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.23      0.15      0.18       124\n",
      "          1       0.84      0.90      0.87      1942\n",
      "          2       0.50      0.37      0.43       412\n",
      "\n",
      "avg / total       0.75      0.78      0.76      2478\n",
      "\n",
      "Label Spreading model: 7560 labeled & 14745 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.25      0.13      0.17       124\n",
      "          1       0.84      0.91      0.87      1942\n",
      "          2       0.51      0.38      0.43       412\n",
      "\n",
      "avg / total       0.75      0.78      0.76      2478\n",
      "\n",
      "Label Spreading model: 8195 labeled & 14110 unlabeled (22305 total)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.25      0.14      0.18       124\n",
      "          1       0.84      0.91      0.87      1942\n",
      "          2       0.52      0.38      0.44       412\n",
      "\n",
      "avg / total       0.76      0.78      0.77      2478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "al_err = []\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    \n",
    "# Outer loop\n",
    "for ind_train, ind_test in kfold.split(X):\n",
    "    print(\"New kfold\")\n",
    "    X_train, y_train, X_test, y_test = X[ind_train], Y[ind_train], X[ind_test], Y[ind_test]\n",
    "    \n",
    "    n_labeled_points = math.ceil(len(y_train)*0.05)\n",
    "    unlabeled_indices = np.arange(len(y_train))[n_labeled_points:]\n",
    "    max_iterations = 100\n",
    "    for i in range(max_iterations):\n",
    "        if len(unlabeled_indices) < 300 :\n",
    "            print(\"No unlabeled data to label.\")\n",
    "            break\n",
    "        \n",
    "        mask = np.ones(X_train.shape[0],dtype=bool) \n",
    "        mask[unlabeled_indices] = False\n",
    "        \n",
    "        \n",
    "        Xal_train = np.copy(X_train[mask])\n",
    "        \n",
    "        yal_train = np.copy(y_train[mask])\n",
    "        \n",
    "        al_model = label_propagation.LabelSpreading(gamma=0.25,max_iter=200)\n",
    "        al_model.fit(Xal_train,yal_train)\n",
    "    \n",
    "        if(i%10==0):\n",
    "            print(\"Label Spreading model: %d labeled & %d unlabeled (%d total)\"\n",
    "            % (len(Xal_train), len(y_train) - len(Xal_train),\n",
    "             len(y_train)))\n",
    "            h = al_model.predict(X_test)\n",
    "    \n",
    "            al_err.append(zero_one_loss(y_test, h))\n",
    "            print(classification_report( y_test, h ))\n",
    "        \n",
    "        pred_entropies = al_model.predict_proba(X_train[~mask])\n",
    "        \n",
    "        uncertainty_index = np.argsort(np.amax(pred_entropies,axis=1))[0:100]\n",
    "        \n",
    "        delete_indices = []\n",
    "    \n",
    "        for index, check_index in enumerate(uncertainty_index):\n",
    "            check = X[check_index]\n",
    "            delete_indices.append(unlabeled_indices[check_index])\n",
    "            \n",
    "        \n",
    "        \n",
    "        unlabeled_indices = np.delete(unlabeled_indices, delete_indices)\n",
    "       \n",
    "        n_labeled_points += len(uncertainty_index)\n",
    "    \n",
    "    al_err.append(zero_one_loss(y_test, h))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f12983fd898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(x=\"class\", data=dataset);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

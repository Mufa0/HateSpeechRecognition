{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mufa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/mufa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/mufa/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialization for HateSpeechRecognition\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import t\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_kfold_cv(clf, param_grid, X, y, k1=10, k2=3):\n",
    "    \n",
    "    err = []\n",
    "    kfold = KFold(n_splits=k1, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Outer loop\n",
    "    for ind_train, ind_test in kfold.split(X):\n",
    "        \n",
    "        X_train, y_train, X_test, y_test = X[ind_train], y[ind_train], X[ind_test], y[ind_test]\n",
    "        \n",
    "        # Inner loop\n",
    "        inn = GridSearchCV(clf,param_grid,n_jobs=5, cv=StratifiedKFold(n_splits=5, \n",
    "                                              random_state=42).split(X_train, y_train), \n",
    "                           verbose=2).fit(X_train, y_train)\n",
    "        \n",
    "        # Prediction based on the best selected params, the ones that minimize average error\n",
    "        h = inn.best_estimator_.fit(X_train, y_train).predict(X_test)\n",
    "        \n",
    "        err.append(zero_one_loss(y_test, h))\n",
    "        print(classification_report( y_test, h ))\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(rgx_list, text):\n",
    "    new_text = text\n",
    "    for rgx_match in rgx_list:\n",
    "        new_text = re.sub(rgx_match, '', new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_words(content):\n",
    "    ret_list = []\n",
    "    fdist2 = nltk.FreqDist(content)\n",
    "    most_list = fdist2.most_common(75)\n",
    "    for x in most_list:\n",
    "        ret_list.append(x)\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "#other_exclusions = [\"#ff\", \"ff\", \"rt\",\"!\",\":\",\"...\",\".\",\"-\",\"&\",\"?\"]\n",
    "#stopwords_list.extend(other_exclusions)\n",
    "\n",
    "dataset = pd.read_csv(\"dataset/labeled_data.csv\")\n",
    "\n",
    "tweets = dataset.tweet\n",
    "\n",
    "ps = nltk.stem.PorterStemmer()\n",
    "\n",
    "#ps.stem(dataset)\n",
    "\n",
    "#Text cleaning and tokenization, then stemming then POS tagging\n",
    "filtered_tweets=[];\n",
    "tweet_tags = [];\n",
    "filtered_tweets_stemmed=[];\n",
    "common_words_prepare=[];\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweet = clean_text([\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\".*@.*:\",\"&#*\\w*\",\"@[\\w\\-]+\",\"[^\\w\\s]\"],tweet)\n",
    "    tweet = tweet.lower()\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    common_words = get_common_words(word_tokens)\n",
    "    for word in common_words:\n",
    "        common_words_prepare.append(word)\n",
    "    filtered_tweets.append([word for word in word_tokens if not word in stopwords_list])\n",
    "    filtered_tweets_stemmed.append([ps.stem(word) for word in word_tokens if not word in stopwords_list])\n",
    "    \n",
    "for tweet in filtered_tweets:\n",
    "    tweet_tags.append(nltk.pos_tag(tweet))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Izbacivanje frequent wordsa\n",
    "\n",
    "common_words = get_common_words(common_words_prepare)\n",
    "#for word in filtered_tweets[1:100]:\n",
    "#    print(word)\n",
    "common = []\n",
    "for word in common_words[1:50]:\n",
    "   common.append(word[0][0])\n",
    "\n",
    "filtered_tweets_no_common = [];\n",
    "filtered_tweets_no_common_stemmed = [];\n",
    "for line in filtered_tweets:\n",
    "    filtered_tweets_no_common.append([word for word in line if not word in common])\n",
    "    filtered_tweets_no_common_stemmed.append([ps.stem(word) for word in line if not word in common])\n",
    "#print(\"---------------------------\")\n",
    "#for word in filtered_tweets_no_common[1:100]:\n",
    "#    print(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating TfIdf vectorizer\n",
    "ftss=[]\n",
    "for tweet in filtered_tweets_no_common_stemmed:\n",
    "    ftss.append(' '.join(tweet))\n",
    "\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(ngram_range=(1,3),lowercase=False,max_features=1000,smooth_idf=False,norm=None,max_df=0.75,min_df=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(ftss).toarray()\n",
    "Y = dataset['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=42, test_size=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 69.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.12      0.19       164\n",
      "          1       0.84      0.97      0.90      1905\n",
      "          2       0.79      0.44      0.56       410\n",
      "\n",
      "avg / total       0.80      0.82      0.80      2479\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 69.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.38      0.16      0.22       127\n",
      "          1       0.84      0.97      0.90      1927\n",
      "          2       0.82      0.42      0.56       425\n",
      "\n",
      "avg / total       0.82      0.83      0.81      2479\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 67.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.10      0.17       136\n",
      "          1       0.84      0.97      0.90      1916\n",
      "          2       0.81      0.45      0.58       427\n",
      "\n",
      "avg / total       0.82      0.84      0.81      2479\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 69.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.14      0.21       145\n",
      "          1       0.85      0.97      0.90      1917\n",
      "          2       0.79      0.48      0.59       416\n",
      "\n",
      "avg / total       0.82      0.84      0.81      2478\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 70.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.16      0.26       171\n",
      "          1       0.83      0.98      0.90      1894\n",
      "          2       0.83      0.41      0.55       413\n",
      "\n",
      "avg / total       0.83      0.83      0.80      2478\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 67.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.15      0.23       141\n",
      "          1       0.85      0.97      0.91      1930\n",
      "          2       0.85      0.49      0.62       407\n",
      "\n",
      "avg / total       0.83      0.85      0.82      2478\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 69.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.16      0.25       154\n",
      "          1       0.84      0.97      0.90      1915\n",
      "          2       0.80      0.45      0.57       409\n",
      "\n",
      "avg / total       0.82      0.83      0.81      2478\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 66.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.13      0.21       135\n",
      "          1       0.84      0.97      0.90      1907\n",
      "          2       0.82      0.45      0.58       436\n",
      "\n",
      "avg / total       0.82      0.83      0.81      2478\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 72.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.13      0.21       133\n",
      "          1       0.86      0.97      0.91      1937\n",
      "          2       0.81      0.49      0.61       408\n",
      "\n",
      "avg / total       0.84      0.85      0.82      2478\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 68.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.42      0.13      0.20       124\n",
      "          1       0.86      0.98      0.91      1942\n",
      "          2       0.86      0.50      0.63       412\n",
      "\n",
      "avg / total       0.84      0.85      0.83      2478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SVM Classifier\n",
    "C = [ 2e-2, 2e-1, 2e-0]\n",
    "gama = [ 2e-2, 2e-1, 2e-0]\n",
    "param = [{'svc__kernel': ['rbf'], 'svc__C': C}]\n",
    "clf = svm.SVC(decision_function_shape='ovo')\n",
    "scale = StandardScaler()\n",
    "pipeline = Pipeline([('scaler', scale), ('svc', clf)])\n",
    "\n",
    "svm_err = nested_kfold_cv(pipeline, param, X, Y)\n",
    "#param_grid = [{}] \n",
    "#grid_search = GridSearchCV(pipeline, \n",
    "#                           param_grid,\n",
    "#                          n_jobs = 5,\n",
    "#                           cv=StratifiedKFold(n_splits=5, \n",
    "#                                              random_state=42).split(X_train, y_train), \n",
    "#                           verbose=2)\n",
    "#model = grid_search.fit(X_train,y_train)\n",
    "#y_pred = model.predict(X_test)\n",
    "#report = classification_report( y_test, y_pred )\n",
    "#print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.10      0.17       164\n",
      "          1       0.85      0.97      0.90      1905\n",
      "          2       0.80      0.54      0.65       410\n",
      "\n",
      "avg / total       0.82      0.84      0.81      2479\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.47      0.12      0.19       127\n",
      "          1       0.86      0.97      0.91      1927\n",
      "          2       0.80      0.50      0.62       425\n",
      "\n",
      "avg / total       0.83      0.85      0.82      2479\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.10      0.17       136\n",
      "          1       0.86      0.97      0.91      1916\n",
      "          2       0.81      0.55      0.66       427\n",
      "\n",
      "avg / total       0.83      0.85      0.83      2479\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.15      0.24       145\n",
      "          1       0.87      0.96      0.91      1917\n",
      "          2       0.80      0.58      0.67       416\n",
      "\n",
      "avg / total       0.84      0.85      0.83      2478\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.13      0.21       171\n",
      "          1       0.84      0.97      0.90      1894\n",
      "          2       0.80      0.50      0.62       413\n",
      "\n",
      "avg / total       0.82      0.83      0.81      2478\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.12      0.20       141\n",
      "          1       0.86      0.97      0.91      1930\n",
      "          2       0.82      0.56      0.66       407\n",
      "\n",
      "avg / total       0.84      0.86      0.83      2478\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.12      0.19       154\n",
      "          1       0.86      0.97      0.91      1915\n",
      "          2       0.79      0.55      0.65       409\n",
      "\n",
      "avg / total       0.82      0.84      0.82      2478\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.41      0.11      0.17       135\n",
      "          1       0.85      0.96      0.90      1907\n",
      "          2       0.80      0.55      0.65       436\n",
      "\n",
      "avg / total       0.82      0.84      0.82      2478\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.11      0.17       133\n",
      "          1       0.86      0.97      0.91      1937\n",
      "          2       0.79      0.55      0.65       408\n",
      "\n",
      "avg / total       0.83      0.85      0.83      2478\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 out of  35 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.17      0.26       124\n",
      "          1       0.88      0.98      0.92      1942\n",
      "          2       0.86      0.58      0.70       412\n",
      "\n",
      "avg / total       0.86      0.87      0.85      2478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "C = [2e-3, 2e-2, 2e-1, 2e-0, 2e-1, 2e-2, 2e-3]\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "clf = LogisticRegression(multi_class='ovr',solver='newton-cg')\n",
    "pipeline = Pipeline([('scaler', std_scaler)  , ('clf', clf)])\n",
    "param = [{'clf__C': C}]\n",
    "\n",
    "logreg_err = nested_kfold_cv(pipeline, param, X, Y)\n",
    "\n",
    "\n",
    "\n",
    "#TODO: logicstic regression, model selection, kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logreg_err' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-768ce2fd6f0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mttest_rel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpaired_t_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mt_stat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaired_t_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogreg_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvm_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp_val\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logreg_err' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_rel as paired_t_test\n",
    "\n",
    "t_stat, p_val = paired_t_test(logreg_err, svm_err)\n",
    "\n",
    "if p_val <= 0.05:\n",
    "    print('\\np-value = ', p_val, ', hipoteza se odbacuje.')\n",
    "else:\n",
    "    print('\\np-value = ', p_val, ', hipoteza se ne odbacuje.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14137</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>People who go to church in Hollywood, CA are a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22158</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>There's apparently a wee cunt who's a year you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22193</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>These bitches is wild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12149</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Is Hillary the &amp;#8216;dodo bird&amp;#8217; candida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13760</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Nose goes on taking out the trash this time. &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1582</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>&amp;#8220;@TedOfficialPage: Fucked your bitch and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14448</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @AdamSank: Why does Julia Roberts seem comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>23730</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>You are a jiggaboo...!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4906</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@SteveStockmanTX hes just a friggin idiot that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19695</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @isminiHoran: &amp;#8220;i&amp;#8217;m running like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10148</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Home to chicken and penne Alfredo and Oreos an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6462</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>@jonsweethearts how are you Argie?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3619</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@ItsKeeshKapeesh *yourself *faggot buy a dicti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>710</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>#Dutch farmers are white trash.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>21686</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Sunday's are the day to chug through while eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14198</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Playing a show in Lutsen, MN at 9:00 PM today ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>13596</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Niccas get a few dollars and start playing the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>188</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>\"@MaxMayo77: http://t.co/3Jk4kR44X3\" a pissed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>192</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>\"@Mesha_nojas: @_Vontethekidd &amp;#128079;&amp;#12807...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>14818</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @BrentLanders6: Charlie: \"My last two songs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>367</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\"@marackaf: &amp;#8220;@white_thunduh: Aye yall im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>17519</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @RollingWithTrin: \"1, 2, 3, 4, how many nig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3109</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>@DinoLich Don't yell at me, I love both of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8462</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Burnating all Yankees and their thatch roof co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20139</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @mjxcx: Teachers trying to pronounce ghetto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7285</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>@steve_phillips_ funny, I thought @bobmuellerw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7409</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>@trix2343 I have something that you don't bitc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>23063</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>What a fag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3040</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@DarienDaywalt bitch shut the fuck up goddam y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>14600</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @BALTsneakerShow: Water is wet. RT @202SOLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4260</th>\n",
       "      <td>4611</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@RabehChararah1 lmaoo what a fag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4261</th>\n",
       "      <td>17788</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @SportsCenter: A WILD one in the Bronx.\\nYa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4262</th>\n",
       "      <td>649</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\"if she can suck your dick with a dip in, she'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4263</th>\n",
       "      <td>9480</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Fuck Ur god until dat faggot strikes me down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4264</th>\n",
       "      <td>18016</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @ThatNiggaKeek: remember in a relationship....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4265</th>\n",
       "      <td>3442</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@HeauxmerSimpson I'm jus tryna vaca away from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4266</th>\n",
       "      <td>22263</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>These hoes got more bodies than a cemetery&amp;#12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4267</th>\n",
       "      <td>8937</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Delusional #teabagger thinks he can stop #gay ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4268</th>\n",
       "      <td>9914</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Happy Birthday nigguh keep your blunts fat and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4269</th>\n",
       "      <td>17163</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @OfficiallyIce: Lmaoooo RT @SourpowerDre: @...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4270</th>\n",
       "      <td>9449</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Freezing my nips off out here...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4271</th>\n",
       "      <td>23879</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>You old bitter,salty ass niccas make my flesh ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4272</th>\n",
       "      <td>13970</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>One man's trash is another's Transformer &amp;#859...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4273</th>\n",
       "      <td>19500</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @halfricannnnn: nigger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4274</th>\n",
       "      <td>8935</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Delete now you bitch ass nigga RT @Quanny_Quan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4275</th>\n",
       "      <td>22556</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>This kid looks like a retard when he tries hid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4276</th>\n",
       "      <td>25015</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>the bird with the aim of dreams https://t.co/j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4277</th>\n",
       "      <td>18964</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @amh_1126: s/o #NF @patpatbush thanks for t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4278</th>\n",
       "      <td>4721</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@RunuldoRants faggot sack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4279</th>\n",
       "      <td>8824</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>DEREK JETER clap clap clapclapclap #Yankees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4280</th>\n",
       "      <td>12605</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>LOLOLOL RT @AjalaPilgrim: Freddy still have al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4281</th>\n",
       "      <td>18570</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @Yannapyt: it ain't nothing to cut that bit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4282</th>\n",
       "      <td>18325</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @USRealityCheck: Republicans Want to Yank F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4283</th>\n",
       "      <td>16740</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @ManiChakr: @ArvindKejriwal itni bahaduri!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4284</th>\n",
       "      <td>5648</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@alexissert leave dyke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4285</th>\n",
       "      <td>19005</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @audizzle: Quote of last night, @GinaGaus t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4286</th>\n",
       "      <td>2356</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>@1Josh_Warwick3 @ashleyypat17 fag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4287</th>\n",
       "      <td>18060</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @TheEagleFazi: Almost 83% of children in Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4288</th>\n",
       "      <td>21424</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>So niggers really believe Phelps just smokin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4289</th>\n",
       "      <td>17633</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @SeoRedondo: If a bitch tries to get crazy ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4290 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0          14137      3            2                   0        1      0   \n",
       "1          22158      6            0                   6        0      1   \n",
       "2          22193      3            0                   3        0      1   \n",
       "3          12149      3            0                   1        2      2   \n",
       "4          13760      3            0                   0        3      2   \n",
       "5           1582      3            0                   3        0      1   \n",
       "6          14448      3            0                   0        3      2   \n",
       "7          23730      3            3                   0        0      0   \n",
       "8           4906      3            3                   0        0      0   \n",
       "9          19695      3            0                   3        0      1   \n",
       "10         10148      3            0                   0        3      2   \n",
       "11          6462      3            0                   0        3      2   \n",
       "12          3619      3            2                   1        0      0   \n",
       "13           710      3            2                   0        1      0   \n",
       "14         21686      3            0                   0        3      2   \n",
       "15         14198      3            0                   0        3      2   \n",
       "16         13596      3            1                   2        0      1   \n",
       "17           188      3            0                   1        2      2   \n",
       "18           192      3            0                   0        3      2   \n",
       "19         14818      3            1                   0        2      2   \n",
       "20           367      3            0                   3        0      1   \n",
       "21         17519      3            3                   0        0      0   \n",
       "22          3109      3            0                   0        3      2   \n",
       "23          8462      3            1                   0        2      2   \n",
       "24         20139      3            0                   1        2      2   \n",
       "25          7285      3            0                   1        2      2   \n",
       "26          7409      3            0                   3        0      1   \n",
       "27         23063      3            3                   0        0      0   \n",
       "28          3040      3            2                   1        0      0   \n",
       "29         14600      3            2                   0        1      0   \n",
       "...          ...    ...          ...                 ...      ...    ...   \n",
       "4260        4611      3            2                   1        0      0   \n",
       "4261       17788      3            0                   0        3      2   \n",
       "4262         649      3            0                   3        0      1   \n",
       "4263        9480      3            2                   1        0      0   \n",
       "4264       18016      3            0                   3        0      1   \n",
       "4265        3442      3            3                   0        0      0   \n",
       "4266       22263      3            1                   2        0      1   \n",
       "4267        8937      3            0                   1        2      2   \n",
       "4268        9914      3            0                   3        0      1   \n",
       "4269       17163      3            1                   2        0      1   \n",
       "4270        9449      3            0                   1        2      2   \n",
       "4271       23879      3            2                   1        0      0   \n",
       "4272       13970      3            0                   0        3      2   \n",
       "4273       19500      3            2                   1        0      0   \n",
       "4274        8935      3            2                   1        0      0   \n",
       "4275       22556      3            2                   1        0      0   \n",
       "4276       25015      3            0                   0        3      2   \n",
       "4277       18964      3            0                   0        3      2   \n",
       "4278        4721      3            2                   1        0      0   \n",
       "4279        8824      3            0                   1        2      2   \n",
       "4280       12605      3            0                   0        3      2   \n",
       "4281       18570      3            0                   3        0      1   \n",
       "4282       18325      3            0                   0        3      2   \n",
       "4283       16740      3            0                   0        3      2   \n",
       "4284        5648      3            2                   1        0      0   \n",
       "4285       19005      3            0                   0        3      2   \n",
       "4286        2356      3            0                   3        0      1   \n",
       "4287       18060      3            0                   0        3      2   \n",
       "4288       21424      3            2                   1        0      0   \n",
       "4289       17633      3            0                   3        0      1   \n",
       "\n",
       "                                                  tweet  \n",
       "0     People who go to church in Hollywood, CA are a...  \n",
       "1     There's apparently a wee cunt who's a year you...  \n",
       "2                                 These bitches is wild  \n",
       "3     Is Hillary the &#8216;dodo bird&#8217; candida...  \n",
       "4     Nose goes on taking out the trash this time. &...  \n",
       "5     &#8220;@TedOfficialPage: Fucked your bitch and...  \n",
       "6     RT @AdamSank: Why does Julia Roberts seem comp...  \n",
       "7                                You are a jiggaboo...!  \n",
       "8     @SteveStockmanTX hes just a friggin idiot that...  \n",
       "9     RT @isminiHoran: &#8220;i&#8217;m running like...  \n",
       "10    Home to chicken and penne Alfredo and Oreos an...  \n",
       "11                   @jonsweethearts how are you Argie?  \n",
       "12    @ItsKeeshKapeesh *yourself *faggot buy a dicti...  \n",
       "13                      #Dutch farmers are white trash.  \n",
       "14    Sunday's are the day to chug through while eve...  \n",
       "15    Playing a show in Lutsen, MN at 9:00 PM today ...  \n",
       "16    Niccas get a few dollars and start playing the...  \n",
       "17    \"@MaxMayo77: http://t.co/3Jk4kR44X3\" a pissed ...  \n",
       "18    \"@Mesha_nojas: @_Vontethekidd &#128079;&#12807...  \n",
       "19    RT @BrentLanders6: Charlie: \"My last two songs...  \n",
       "20    \"@marackaf: &#8220;@white_thunduh: Aye yall im...  \n",
       "21    RT @RollingWithTrin: \"1, 2, 3, 4, how many nig...  \n",
       "22    @DinoLich Don't yell at me, I love both of the...  \n",
       "23    Burnating all Yankees and their thatch roof co...  \n",
       "24    RT @mjxcx: Teachers trying to pronounce ghetto...  \n",
       "25    @steve_phillips_ funny, I thought @bobmuellerw...  \n",
       "26    @trix2343 I have something that you don't bitc...  \n",
       "27                                           What a fag  \n",
       "28    @DarienDaywalt bitch shut the fuck up goddam y...  \n",
       "29    RT @BALTsneakerShow: Water is wet. RT @202SOLE...  \n",
       "...                                                 ...  \n",
       "4260                   @RabehChararah1 lmaoo what a fag  \n",
       "4261  RT @SportsCenter: A WILD one in the Bronx.\\nYa...  \n",
       "4262  \"if she can suck your dick with a dip in, she'...  \n",
       "4263       Fuck Ur god until dat faggot strikes me down  \n",
       "4264  RT @ThatNiggaKeek: remember in a relationship....  \n",
       "4265  @HeauxmerSimpson I'm jus tryna vaca away from ...  \n",
       "4266  These hoes got more bodies than a cemetery&#12...  \n",
       "4267  Delusional #teabagger thinks he can stop #gay ...  \n",
       "4268  Happy Birthday nigguh keep your blunts fat and...  \n",
       "4269  RT @OfficiallyIce: Lmaoooo RT @SourpowerDre: @...  \n",
       "4270                   Freezing my nips off out here...  \n",
       "4271  You old bitter,salty ass niccas make my flesh ...  \n",
       "4272  One man's trash is another's Transformer &#859...  \n",
       "4273                          RT @halfricannnnn: nigger  \n",
       "4274  Delete now you bitch ass nigga RT @Quanny_Quan...  \n",
       "4275  This kid looks like a retard when he tries hid...  \n",
       "4276  the bird with the aim of dreams https://t.co/j...  \n",
       "4277  RT @amh_1126: s/o #NF @patpatbush thanks for t...  \n",
       "4278                          @RunuldoRants faggot sack  \n",
       "4279        DEREK JETER clap clap clapclapclap #Yankees  \n",
       "4280  LOLOLOL RT @AjalaPilgrim: Freddy still have al...  \n",
       "4281  RT @Yannapyt: it ain't nothing to cut that bit...  \n",
       "4282  RT @USRealityCheck: Republicans Want to Yank F...  \n",
       "4283  RT @ManiChakr: @ArvindKejriwal itni bahaduri!!...  \n",
       "4284                             @alexissert leave dyke  \n",
       "4285  RT @audizzle: Quote of last night, @GinaGaus t...  \n",
       "4286                  @1Josh_Warwick3 @ashleyypat17 fag  \n",
       "4287  RT @TheEagleFazi: Almost 83% of children in Ba...  \n",
       "4288  So niggers really believe Phelps just smokin o...  \n",
       "4289  RT @SeoRedondo: If a bitch tries to get crazy ...  \n",
       "\n",
       "[4290 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sampling data so we get equal distribution for all classes. used in active learning\n",
    "d0 = dataset[dataset['class'] == 0]\n",
    "d1 = dataset[dataset['class'] == 1].sample(d0.shape[0])\n",
    "d2 = dataset[dataset['class'] == 2].sample(d0.shape[0])\n",
    "\n",
    "#Dl data that is representing same number of all classes. From this we will take n data for starter training as\n",
    "#labeled data, and rest for unlabeled data for active learning\n",
    "Dl = pd.concat([d0,d1,d2])\n",
    "Dl = sklearn.utils.shuffle(Dl)\n",
    "Dl.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_filtered_tweets=[];\n",
    "AL_tweet_tags = [];\n",
    "AL_filtered_tweets_stemmed=[];\n",
    "AL_common_words_prepare=[];\n",
    "\n",
    "for tweet in Dl['tweet']:\n",
    "    tweet = clean_text([\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\".*@.*:\",\"&#*\\w*\",\"@[\\w\\-]+\",\"[^\\w\\s]\"],tweet)\n",
    "    tweet = tweet.lower()\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    common_words = get_common_words(word_tokens)\n",
    "    for word in common_words:\n",
    "        AL_common_words_prepare.append(word)\n",
    "    AL_filtered_tweets.append([word for word in word_tokens if not word in stopwords_list])\n",
    "    AL_filtered_tweets_stemmed.append([ps.stem(word) for word in word_tokens if not word in stopwords_list])\n",
    "    \n",
    "for tweet in AL_filtered_tweets:\n",
    "    AL_tweet_tags.append(nltk.pos_tag(tweet))\n",
    "\n",
    "# Izbacivanje frequent wordsa\n",
    "\n",
    "AL_common_words = get_common_words(AL_common_words_prepare)\n",
    "#for word in filtered_tweets[1:100]:\n",
    "#    print(word)\n",
    "AL_common = []\n",
    "for word in AL_common_words[1:50]:\n",
    "   AL_common.append(word[0][0])\n",
    "\n",
    "AL_filtered_tweets_no_common = [];\n",
    "AL_filtered_tweets_no_common_stemmed = [];\n",
    "for line in AL_filtered_tweets:\n",
    "    AL_filtered_tweets_no_common.append([word for word in line if not word in AL_common])\n",
    "    AL_filtered_tweets_no_common_stemmed.append([ps.stem(word) for word in line if not word in AL_common])\n",
    "#print(\"---------------------------\")\n",
    "#for word in filtered_tweets_no_common[1:100]:\n",
    "#    print(word)\n",
    "\n",
    "AL_ftss=[]\n",
    "for tweet in AL_filtered_tweets_no_common_stemmed:\n",
    "    AL_ftss.append(' '.join(tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_X = vectorizer.fit_transform(AL_ftss).toarray()\n",
    "AL_Y = Dl['class'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "Tk_X, Tu_X, Tk_y, _ = train_test_split(AL_X, AL_Y, random_state=42, test_size=0.8)\n",
    "\n",
    "n_labeled_points = math.ceil(len(AL_Y)*0.05)\n",
    "unlabeled_indices = np.arange(len(AL_Y))[n_labeled_points:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mufa/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:52: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb1d3e31a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.semi_supervised import label_propagation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "max_iterations = 100\n",
    "for i in range(max_iterations):\n",
    "    if len(Tu_X) == 0 :\n",
    "        print(\"No unlabeled data to label.\")\n",
    "        break\n",
    "    al_model = label_propagation.LabelSpreading(gamma=0.25,max_iter=200)\n",
    "    y_train = np.copy(AL_Y)\n",
    "    y_train[unlabeled_indices] = -1\n",
    "    al_model.fit(AL_X,y_train)\n",
    "    predicted_labels = al_model.transduction_[unlabeled_indices]\n",
    "    true_labels = [AL_Y[i] for i in unlabeled_indices]\n",
    "    #if set(al_model.classes_).issubset(set(true_labels)) == False:\n",
    "    #    break\n",
    "    #cm = confusion_matrix(true_labels, predicted_labels,\n",
    "    #                      labels=al_model.classes_)\n",
    "    #print(\"Iteration %i %s\" % (i, 70 * \"_\"))\n",
    "    \n",
    "        \n",
    "    #print(\"Label Spreading model: %d labeled & %d unlabeled (%d total)\"\n",
    "    #      % (n_labeled_points, len(AL_Y) - n_labeled_points,\n",
    "    #         len(AL_Y)))\n",
    "\n",
    "    #print(classification_report(true_labels, predicted_labels))\n",
    "\n",
    "    #print(\"Confusion matrix\")\n",
    "    #print(cm)\n",
    "\n",
    "    # compute the entropies of transduced label distributions\n",
    "    pred_entropies = stats.distributions.entropy(\n",
    "        al_model.label_distributions_.T)\n",
    "\n",
    "    # select up to 5 digit examples that the classifier is most uncertain about\n",
    "    uncertainty_index = np.argsort(pred_entropies)[::-1]\n",
    "    uncertainty_index = uncertainty_index[\n",
    "        np.in1d(uncertainty_index, unlabeled_indices)][:50]\n",
    "\n",
    "    # keep track of indices that we get labels for\n",
    "    delete_indices = np.array([])\n",
    "    \n",
    "    for index, check_index in enumerate(uncertainty_index):\n",
    "        check = AL_ftss[check_index]\n",
    "\n",
    "        delete_index, = np.where(unlabeled_indices == check_index)\n",
    "        delete_indices = np.concatenate((delete_indices, delete_index))\n",
    "\n",
    "    unlabeled_indices = np.delete(unlabeled_indices, delete_indices)\n",
    "    n_labeled_points += len(uncertainty_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-6df2fd758846>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0myal_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munlabeled_indices\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myal_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/semi_supervised/label_propagation.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    410\u001b[0m             )\n\u001b[1;32m    411\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLabelPropagation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/semi_supervised/label_propagation.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;31m# actual graph construction (implementations should override this)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mgraph_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# label construction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/semi_supervised/label_propagation.py\u001b[0m in \u001b[0;36m_build_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'knn'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         \u001b[0maffinity_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         \u001b[0mnormalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maffinity_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maffinity_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/semi_supervised/label_propagation.py\u001b[0m in \u001b[0;36m_get_kernel\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"rbf\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mrbf_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mrbf_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mrbf_kernel\u001b[0;34m(X, Y, gamma)\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m     \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m     \u001b[0mK\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# exponentiate K in-place\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mYY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m     \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "al_err = []\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    \n",
    "# Outer loop\n",
    "for ind_train, ind_test in kfold.split(X):\n",
    "        \n",
    "    X_train, y_train, X_test, y_test = X[ind_train], Y[ind_train], X[ind_test], Y[ind_test]\n",
    "        \n",
    "    # Prediction based on the best selected params, the ones that minimize average error\n",
    "    h = al_model.predict(X_test)\n",
    "        \n",
    "    al_err.append(zero_one_loss(y_test, h))\n",
    "    print(classification_report( y_test, h ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

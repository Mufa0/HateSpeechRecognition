{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mufa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/mufa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/mufa/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialization for HateSpeechRecognition\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(rgx_list, text):\n",
    "    new_text = text\n",
    "    for rgx_match in rgx_list:\n",
    "        new_text = re.sub(rgx_match, '', new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "#other_exclusions = [\"#ff\", \"ff\", \"rt\",\"!\",\":\",\"...\",\".\",\"-\",\"&\",\"?\"]\n",
    "#stopwords_list.extend(other_exclusions)\n",
    "\n",
    "dataset = pd.read_csv(\"dataset/labeled_data.csv\")\n",
    "\n",
    "tweets = dataset.tweet\n",
    "\n",
    "ps = nltk.stem.PorterStemmer()\n",
    "\n",
    "#ps.stem(dataset)\n",
    "\n",
    "#Text cleaning and tokenization, then stemming then POS tagging\n",
    "filtered_tweets=[];\n",
    "tweet_tags = [];\n",
    "filtered_tweets_stemmed=[];\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweet = clean_text([\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\".*@.*:\",\"&#*\\w*\",\"@[\\w\\-]+\",\"[^\\w\\s]\"],tweet)\n",
    "    tweet = tweet.lower()\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    filtered_tweets.append([word for word in word_tokens if not word in stopwords_list])\n",
    "    filtered_tweets_stemmed.append([ps.stem(word) for word in word_tokens if not word in stopwords_list])\n",
    "    \n",
    "for tweet in filtered_tweets:\n",
    "    tweet_tags.append(nltk.pos_tag(tweet))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating TfIdf vectorizer\n",
    "ftss=[]\n",
    "for tweet in filtered_tweets_stemmed:\n",
    "    ftss.append(' '.join(tweet))\n",
    "clf = svm.SVC(decision_function_shape='ovo')\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(ngram_range=(1,3),lowercase=False,max_features=10000,smooth_idf=False,norm=None,max_df=0.75,min_df=5)\n",
    "X = vectorizer.fit_transform(ftss).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = dataset['class']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=42, test_size=0.3)\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 2 1 1]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.01      0.01       427\n",
      "          1       0.86      0.98      0.92      5747\n",
      "          2       0.85      0.57      0.68      1261\n",
      "\n",
      "avg / total       0.85      0.86      0.82      7435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report( y_test, y_pred )\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
